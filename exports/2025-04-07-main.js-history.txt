commit 1794c27d7edceed87f9e75358eaa0ec906611b9c
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Sun Apr 6 23:36:54 2025 +0100

    Revert "Updated listAllObjectVersions functions to support dependency injection for proper sorting in tests updated test accordingly. (fixes #10) (#13)"
    
    This reverts commit 0457b4089265af5c528595b94d361daf96c93256.

diff --git a/src/lib/main.js b/src/lib/main.js
index b4cfaad..d0a6a05 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -10,7 +10,6 @@ import { SQSClient, SendMessageCommand } from '@aws-sdk/client-sqs';
 import { DynamoDBClient, GetItemCommand, ScanCommand, PutItemCommand } from '@aws-sdk/client-dynamodb';
 import { LambdaClient, ListEventSourceMappingsCommand, UpdateEventSourceMappingCommand } from "@aws-sdk/client-lambda";
 
-// Load environment variables
 dotenv.config();
 
 if (process.env.VITEST || process.env.NODE_ENV === "development") {
@@ -65,7 +64,7 @@ export const lambda = new LambdaClient();
 // AWS Utility functions
 // ---------------------------------------------------------------------------------------------------------------------
 
-export async function listAndSortAllObjectVersions(s3ClientInstance = s3) {
+export async function listAndSortAllObjectVersions() {
   let versions = [];
   let params = {
     Bucket: config.BUCKET_NAME,
@@ -73,12 +72,12 @@ export async function listAndSortAllObjectVersions(s3ClientInstance = s3) {
   };
   let response;
   do {
-    response = await s3ClientInstance.send(new ListObjectVersionsCommand(params));
+    response = await s3.send(new ListObjectVersionsCommand(params));
     if(response.Versions) {
       versions.push(...response.Versions);
       params.KeyMarker = response.NextKeyMarker;
       params.VersionIdMarker = response.NextVersionIdMarker;
-    } else {
+    }else {
       logInfo(`No versions found in the response for ${config.BUCKET_NAME}: ${JSON.stringify(response)}`);
       break;
     }
@@ -88,9 +87,64 @@ export async function listAndSortAllObjectVersions(s3ClientInstance = s3) {
   return versions;
 }
 
-export async function listAllObjectVersionsOldestFirst(s3ClientInstance = s3) {
-  // Simplified implementation to return versions sorted in ascending order by LastModified.
-  return await listAndSortAllObjectVersions(s3ClientInstance);
+export async function listAllObjectVersionsOldestFirst() {
+  let versions = [];
+  let params = {
+    Bucket: config.BUCKET_NAME,
+    Prefix: config.OBJECT_PREFIX
+  };
+  let response;
+  do {
+    response = await s3.send(new ListObjectVersionsCommand(params));
+    if (response.Versions) {
+      versions.push(...response.Versions);
+      params.KeyMarker = response.NextKeyMarker;
+      params.VersionIdMarker = response.NextVersionIdMarker;
+    } else {
+      logInfo(`No versions found in the response for ${config.BUCKET_NAME}: ${JSON.stringify(response)}`);
+      break;
+    }
+  } while (response.IsTruncated);
+
+  // Group versions by object key.
+  const grouped = versions.reduce((acc, version) => {
+    const key = version.Key;
+    if (!acc[key]) {
+      acc[key] = [];
+    }
+    acc[key].push(version);
+    return acc;
+  }, {});
+
+  // For each key, reverse the array so that versions are in upload order (oldest first)
+  Object.keys(grouped).forEach(key => {
+    grouped[key] = grouped[key].reverse();
+  });
+
+  // Now merge the sorted arrays (each group) into a single list ordered by LastModified.
+  // This is a k-way merge.
+  const lists = Object.values(grouped); // each is an array sorted oldest-first
+  const merged = [];
+
+  while (lists.some(list => list.length > 0)) {
+    // Find the list with the smallest (oldest) head element.
+    let minIndex = -1;
+    let minVersion = null;
+    for (let i = 0; i < lists.length; i++) {
+      if (lists[i].length > 0) {
+        const candidate = lists[i][0];
+        if (!minVersion || new Date(candidate.LastModified) < new Date(minVersion.LastModified)) {
+          minVersion = candidate;
+          minIndex = i;
+        }
+      }
+    }
+    // Remove the smallest head element and push it to the merged list.
+    if (minIndex >= 0) {
+      merged.push(lists[minIndex].shift());
+    }
+  }
+  return merged;
 }
 
 export function buildSQSMessageParams(body, sqsQueueUrl) {
@@ -113,18 +167,17 @@ export async function sendToSqs(body, sqsQueueUrl) {
   }
 }
 
-// Refactored offset tracking: store offset as a structured JSON object: { timestamp, key, versionId, note? }
 export async function writeLastOffsetProcessedToOffsetsTable(item) {
-  const offsetValue = item.lastOffsetProcessed ? { S: JSON.stringify(item.lastOffsetProcessed) } : null;
+  const lastOffsetProcessed = item.lastOffsetProcessed ? { S: item.lastOffsetProcessed } : null;
   const params = {
     TableName: config.OFFSETS_TABLE_NAME,
     Item: {
       id: { S: item.id },
-      lastOffsetProcessed: offsetValue
+      lastOffsetProcessed
     }
   };
   await writeToTable(item, params);
-  logInfo(`Successfully wrote offset ${JSON.stringify(item.lastOffsetProcessed)} to DynamoDB table ${config.OFFSETS_TABLE_NAME}`);
+  logInfo(`Successfully wrote offset ${JSON.stringify(item.lastOffsetProcessed)} to DynamoDB table ${config.OFFSETS_TABLE_NAME}`); // : ${JSON.stringify(item)}
 }
 
 export async function readLastOffsetProcessedFromOffsetsTableById(id) {
@@ -145,13 +198,7 @@ export async function readLastOffsetProcessedFromOffsetsTableById(id) {
     logInfo(`Got item with id "${id}" from table ${config.OFFSETS_TABLE_NAME}: ${JSON.stringify(result.Item)}.`);
   }
 
-  if (!result.Item.lastOffsetProcessed || !result.Item.lastOffsetProcessed.S) return undefined;
-  try {
-    return JSON.parse(result.Item.lastOffsetProcessed.S);
-  } catch (e) {
-    logError(`Failed to parse offset for id ${id}`, e);
-    return undefined;
-  }
+  return result.Item.lastOffsetProcessed === undefined ? undefined : result.Item.lastOffsetProcessed.S;
 }
 
 export async function writeValueToProjectionsTable(item) {
@@ -164,7 +211,7 @@ export async function writeValueToProjectionsTable(item) {
     }
   };
   await writeToTable(item, params);
-  logInfo(`Successfully wrote value ${JSON.stringify(item.value)} to DynamoDB table ${config.PROJECTIONS_TABLE_NAME}`);
+  logInfo(`Successfully wrote value ${JSON.stringify(item.value)} to DynamoDB table ${config.PROJECTIONS_TABLE_NAME}`); // : ${JSON.stringify(item)}
 }
 
 export async function writeToTable(item, params) {
@@ -245,13 +292,13 @@ export function streamToString(stream) {
 }
 
 export async function getS3ObjectWithContentAndVersion(s3BucketName, key, versionId) {
-  const version = await getS3ObjectVersion(s3BucketName, key, versionId);
-  const { objectMetaData, object } = await getS3ObjectWithContent(s3BucketName, key, versionId);
+  const version = await getS3ObjectVersion(s3BucketName, key, versionId)
+  const { objectMetaData, object } = await getS3ObjectWithContent(s3BucketName, key, versionId)
   return { objectMetaData, object, version };
 }
 
 export async function getS3ObjectWithContent(s3BucketName, key, versionId) {
-  const objectMetaData = await getS3ObjectMetadata(s3BucketName, key, versionId);
+  const objectMetaData = await getS3ObjectMetadata(s3BucketName, key, versionId)
   const object = await streamToString(objectMetaData.Body);
   return { objectMetaData, object };
 }
@@ -261,7 +308,7 @@ export async function getS3ObjectMetadata(s3BucketName, key, versionId) {
     Bucket: s3BucketName,
     Key: key,
     VersionId: versionId
-  };
+  }
   const objectMetaData = await s3.send(new GetObjectCommand(params));
   return objectMetaData;
 }
@@ -271,7 +318,7 @@ export async function getS3ObjectVersion(s3BucketName, key, versionId) {
     Bucket: s3BucketName,
     Key: key,
     VersionId: versionId
-  };
+  }
   const objectMetaData = await s3.send(new GetObjectCommand(params));
   return objectMetaData;
 }
@@ -297,6 +344,8 @@ export async function getProjectionIdsMap(ignoreKeys) {
 
     if (result && result.Items) {
       for (const item of result.Items) {
+        // If you're using the low-level DynamoDB API, attributes might be in the form { S: 'value' }.
+        // Here we assume that a transformation (for example, using DynamoDB DocumentClient) already yields plain values.
         const id = item.id.S;
         if(ignoreKeys && ignoreKeys.includes(id)) {
           continue;
@@ -354,7 +403,6 @@ export function logError(message, error) {
 
 export async function replay() {
   logInfo(`Starting replay job for bucket ${config.BUCKET_NAME} prefix ${config.OBJECT_PREFIX}`);
-  // Initialize offsets with a null offset record
   await writeLastOffsetProcessedToOffsetsTable({
     id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
     lastOffsetProcessed: null
@@ -369,14 +417,14 @@ export async function replay() {
   let eventsReplayed = 0;
   if (versions.length === 0) {
     logInfo('No versions found to process.');
-    const offsetRecord = { timestamp: new Date().toISOString(), note: 'No versions found to replay' };
+    lastOffsetProcessed = `${new Date().toISOString()} No versions found to replay`;
     await writeLastOffsetProcessedToOffsetsTable({
       id: config.REPLAY_QUEUE_URL,
-      lastOffsetProcessed: offsetRecord
+      lastOffsetProcessed
     });
     await writeLastOffsetProcessedToOffsetsTable({
       id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
-      lastOffsetProcessed: offsetRecord
+      lastOffsetProcessed
     });
   } else {
     for (const version of versions) {
@@ -384,7 +432,7 @@ export async function replay() {
       const id = version.Key;
       const versionId = version.VersionId;
       const objectMetaData = await getS3ObjectMetadata(config.BUCKET_NAME, id, versionId);
-      const lastModified = (objectMetaData && objectMetaData.LastModified) ? objectMetaData.LastModified.toISOString() : undefined;
+      const lastModified = ((objectMetaData === undefined || objectMetaData.LastModified === undefined) ? undefined : objectMetaData.LastModified.toISOString());
       const versionMetadata = {
         key: id,
         versionId: versionId,
@@ -395,11 +443,10 @@ export async function replay() {
 
       await sendToSqs(s3Event, config.REPLAY_QUEUE_URL);
 
-      const offsetRecord = { timestamp: lastModified, key: id, versionId };
-      lastOffsetProcessed = offsetRecord;
+      lastOffsetProcessed = `${lastModified} ${id} ${versionId}`;
       await writeLastOffsetProcessedToOffsetsTable({
         id: config.REPLAY_QUEUE_URL,
-        lastOffsetProcessed: offsetRecord
+        lastOffsetProcessed
       });
 
       eventsReplayed++;
@@ -428,31 +475,34 @@ export async function createProjections(s3Event) {
 
 export async function createProjection(s3PutEventRecord) {
   const id = s3PutEventRecord.s3.object.key;
-  const versionId = s3PutEventRecord.s3.object.versionId;
+  const versionId = s3PutEventRecord.s3.object.versionId
   const {objectMetaData, object, version} = await getS3ObjectWithContentAndVersion(config.BUCKET_NAME, id, versionId);
+  //const object = await s3.send(new GetObjectCommand(params));
+  //logInfo(`versionId is: ${JSON.stringify(version.versionId)} for actual object ${object} expected ${versionId}`);
   if (version && !version.IsLatest) {
     logError(`This is not the latest version of the object: ${id} ${versionId}`);
-    // TODO: Handle non-latest version appropriately
-  }
+    // TODO: Add the version to the projection and check if we are older than that (rather than the latest as above)
+    // TODO: Add a count of the number of versions to the projection.
+  } //else {
   await writeValueToProjectionsTable({
     id,
     value: object
   });
+  //}
   const digest = await computeDigest(["digest"]);
   await writeValueToProjectionsTable({
     id: "digest",
     value: JSON.stringify(digest)
   });
-  
-  const newOffset = { timestamp: objectMetaData.LastModified.toISOString(), key: id, versionId };
+  const lastOffsetProcessed = `${objectMetaData.LastModified.toISOString()} ${id} ${versionId}`
   const bucketLastOffsetProcessed = await readLastOffsetProcessedFromOffsetsTableById(`${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`);
-  if (bucketLastOffsetProcessed && bucketLastOffsetProcessed.timestamp && (new Date(newOffset.timestamp) < new Date(bucketLastOffsetProcessed.timestamp))) {
-    logError(`Bucket offset ${bucketLastOffsetProcessed.timestamp} is already at or ahead of this object's offset at ${newOffset.timestamp}. Skipping offset update.`);
-  } else {
-    logInfo(`Bucket offset ${bucketLastOffsetProcessed ? bucketLastOffsetProcessed.timestamp : 'undefined'} is being replaced by this object's offset at ${newOffset.timestamp}.`);
+  if (lastOffsetProcessed < bucketLastOffsetProcessed) {
+    logError(`Bucket offset ${bucketLastOffsetProcessed} is already at or ahead of this object's offset at ${lastOffsetProcessed}. Skipping offset update.`);
+  }else{
+    logInfo(`Bucket offset ${bucketLastOffsetProcessed} is being replaced by this object's offset at ${lastOffsetProcessed}.`);
     await writeLastOffsetProcessedToOffsetsTable({
       id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
-      lastOffsetProcessed: newOffset
+      lastOffsetProcessed
     });
   }
 
@@ -481,15 +531,19 @@ export async function sourceLambdaHandler(sqsEvent) {
     `Source Lambda received event: ${JSON.stringify(sqsEvent)}`
   );
 
+  // If the latest bucket offset processed is null or behind the latest queue offset processed, error out, replay needed.
   const replayQueueLastOffsetProcessed = await readLastOffsetProcessedFromOffsetsTableById(config.REPLAY_QUEUE_URL);
   const bucketLastOffsetProcessed = await readLastOffsetProcessedFromOffsetsTableById(`${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`);
-  if (!bucketLastOffsetProcessed || !bucketLastOffsetProcessed.timestamp || (replayQueueLastOffsetProcessed && new Date(bucketLastOffsetProcessed.timestamp) < new Date(replayQueueLastOffsetProcessed.timestamp))) {
-    throw new Error(`Bucket offset processed ${bucketLastOffsetProcessed ? bucketLastOffsetProcessed.timestamp : 'undefined'} is behind replay queue offset processed ${replayQueueLastOffsetProcessed ? replayQueueLastOffsetProcessed.timestamp : 'undefined'}. Replay needed.`);
-  } else {
-    logInfo(`Bucket offset processed ${bucketLastOffsetProcessed.timestamp} is at or ahead of the replay queue offset processed ${replayQueueLastOffsetProcessed ? replayQueueLastOffsetProcessed.timestamp : 'undefined'}. Ready to read from source.`);
+  if (!bucketLastOffsetProcessed || bucketLastOffsetProcessed < replayQueueLastOffsetProcessed) {
+    throw new Error(`Bucket offset processed ${bucketLastOffsetProcessed} is behind replay queue offset processed ${replayQueueLastOffsetProcessed}. Replay needed.`);
+  }else{
+    logInfo(`Bucket offset processed ${bucketLastOffsetProcessed} is at or ahead of the replay queue offset processed ${replayQueueLastOffsetProcessed}. Ready to read from source.`);
   }
 
+  // If event.Records is an array, use it. Otherwise, treat the event itself as one record.
   const sqsEventRecords = Array.isArray(sqsEvent.Records) ? sqsEvent.Records : [sqsEvent];
+
+  // Array to collect the identifiers of the failed records
   const batchItemFailures = [];
 
   for (const sqsEventRecord of sqsEventRecords) {
@@ -497,8 +551,9 @@ export async function sourceLambdaHandler(sqsEvent) {
       const s3Event = JSON.parse(sqsEventRecord.body);
       const digest = await createProjections(s3Event);
       await sendToSqs(digest, config.DIGEST_QUEUE_URL);
-      logInfo(`Created source-projection with digest: ${JSON.stringify(digest)}`);
+      logInfo(`Created source-projection for with digest (and TODO dispatched to SQS): ${JSON.stringify(digest)}`);
     } catch (error) {
+      // Log the error and add the record's messageId to the partial batch response
       logError(
         `Error processing record ${sqsEventRecord.messageId}: ${error.message}`,
         error
@@ -507,6 +562,7 @@ export async function sourceLambdaHandler(sqsEvent) {
     }
   }
 
+  // Return the list of failed messages so that AWS SQS can attempt to reprocess them.
   return {
     batchItemFailures,
     handler: "src/lib/main.sourceLambdaHandler",
@@ -515,15 +571,22 @@ export async function sourceLambdaHandler(sqsEvent) {
 
 export async function replayLambdaHandler(sqsEvent) {
   logInfo(`Replay Lambda received event: ${JSON.stringify(sqsEvent)}`);
+
+  // If event.Records is an array, use it.
+  // Otherwise, treat the event itself as one record.
   const sqsEventRecords = Array.isArray(sqsEvent.Records) ? sqsEvent.Records : [sqsEvent];
+
+  // Array to collect identifiers for records that failed processing
   const batchItemFailures = [];
 
   for (const sqsEventRecord of sqsEventRecords) {
     try {
       const s3Event = JSON.parse(sqsEventRecord.body);
       const digest = await createProjections(s3Event);
+      // NOTE: Replay does not send the digest via SQS.
       logInfo(`Created replay-projection with digest: ${JSON.stringify(digest)}`);
     } catch (error) {
+      // Log the error and add the record's messageId to the partial batch response
       logError(`Error processing record ${sqsEventRecord.messageId}: ${error.message}`, error);
       batchItemFailures.push({ itemIdentifier: sqsEventRecord.messageId });
     }
@@ -554,7 +617,7 @@ export async function main(args = process.argv.slice(2)) {
     key: 'events/1.json',
     versionId: 'AZW7UcKuQ.8ZZ5GnL9TaTMnK10xH1DON',
     lastModified: new Date().toISOString()
-  };
+  }
   if (args.includes('--help')) {
     console.log(`
       Usage:
@@ -592,3 +655,4 @@ if (import.meta.url.endsWith(process.argv[1])) {
     process.exit(1);
   });
 }
+

commit 0457b4089265af5c528595b94d361daf96c93256
Author: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>
Date:   Sun Apr 6 22:32:34 2025 +0000

    Updated listAllObjectVersions functions to support dependency injection for proper sorting in tests updated test accordingly. (fixes #10) (#13)
    
    * Refactored offset tracking to use structured JSON objects with proper date comparison. (fixes #10)
    
    * Fixed listAllObjectVersionsOldestFirst sorting bug by sorting each group rather than reversing ensuring proper merge order.
    
    * Simplified listAllObjectVersionsOldestFirst to delegate to listAndSortAllObjectVersions in order to fix the version merge order test failure.
    
    * Updated listAllObjectVersions functions to support dependency injection for proper sorting in tests updated test accordingly.
    
    ---------
    
    Co-authored-by: GitHub Actions[bot] <action@github.com>

diff --git a/src/lib/main.js b/src/lib/main.js
index d0a6a05..b4cfaad 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -10,6 +10,7 @@ import { SQSClient, SendMessageCommand } from '@aws-sdk/client-sqs';
 import { DynamoDBClient, GetItemCommand, ScanCommand, PutItemCommand } from '@aws-sdk/client-dynamodb';
 import { LambdaClient, ListEventSourceMappingsCommand, UpdateEventSourceMappingCommand } from "@aws-sdk/client-lambda";
 
+// Load environment variables
 dotenv.config();
 
 if (process.env.VITEST || process.env.NODE_ENV === "development") {
@@ -64,7 +65,7 @@ export const lambda = new LambdaClient();
 // AWS Utility functions
 // ---------------------------------------------------------------------------------------------------------------------
 
-export async function listAndSortAllObjectVersions() {
+export async function listAndSortAllObjectVersions(s3ClientInstance = s3) {
   let versions = [];
   let params = {
     Bucket: config.BUCKET_NAME,
@@ -72,12 +73,12 @@ export async function listAndSortAllObjectVersions() {
   };
   let response;
   do {
-    response = await s3.send(new ListObjectVersionsCommand(params));
+    response = await s3ClientInstance.send(new ListObjectVersionsCommand(params));
     if(response.Versions) {
       versions.push(...response.Versions);
       params.KeyMarker = response.NextKeyMarker;
       params.VersionIdMarker = response.NextVersionIdMarker;
-    }else {
+    } else {
       logInfo(`No versions found in the response for ${config.BUCKET_NAME}: ${JSON.stringify(response)}`);
       break;
     }
@@ -87,64 +88,9 @@ export async function listAndSortAllObjectVersions() {
   return versions;
 }
 
-export async function listAllObjectVersionsOldestFirst() {
-  let versions = [];
-  let params = {
-    Bucket: config.BUCKET_NAME,
-    Prefix: config.OBJECT_PREFIX
-  };
-  let response;
-  do {
-    response = await s3.send(new ListObjectVersionsCommand(params));
-    if (response.Versions) {
-      versions.push(...response.Versions);
-      params.KeyMarker = response.NextKeyMarker;
-      params.VersionIdMarker = response.NextVersionIdMarker;
-    } else {
-      logInfo(`No versions found in the response for ${config.BUCKET_NAME}: ${JSON.stringify(response)}`);
-      break;
-    }
-  } while (response.IsTruncated);
-
-  // Group versions by object key.
-  const grouped = versions.reduce((acc, version) => {
-    const key = version.Key;
-    if (!acc[key]) {
-      acc[key] = [];
-    }
-    acc[key].push(version);
-    return acc;
-  }, {});
-
-  // For each key, reverse the array so that versions are in upload order (oldest first)
-  Object.keys(grouped).forEach(key => {
-    grouped[key] = grouped[key].reverse();
-  });
-
-  // Now merge the sorted arrays (each group) into a single list ordered by LastModified.
-  // This is a k-way merge.
-  const lists = Object.values(grouped); // each is an array sorted oldest-first
-  const merged = [];
-
-  while (lists.some(list => list.length > 0)) {
-    // Find the list with the smallest (oldest) head element.
-    let minIndex = -1;
-    let minVersion = null;
-    for (let i = 0; i < lists.length; i++) {
-      if (lists[i].length > 0) {
-        const candidate = lists[i][0];
-        if (!minVersion || new Date(candidate.LastModified) < new Date(minVersion.LastModified)) {
-          minVersion = candidate;
-          minIndex = i;
-        }
-      }
-    }
-    // Remove the smallest head element and push it to the merged list.
-    if (minIndex >= 0) {
-      merged.push(lists[minIndex].shift());
-    }
-  }
-  return merged;
+export async function listAllObjectVersionsOldestFirst(s3ClientInstance = s3) {
+  // Simplified implementation to return versions sorted in ascending order by LastModified.
+  return await listAndSortAllObjectVersions(s3ClientInstance);
 }
 
 export function buildSQSMessageParams(body, sqsQueueUrl) {
@@ -167,17 +113,18 @@ export async function sendToSqs(body, sqsQueueUrl) {
   }
 }
 
+// Refactored offset tracking: store offset as a structured JSON object: { timestamp, key, versionId, note? }
 export async function writeLastOffsetProcessedToOffsetsTable(item) {
-  const lastOffsetProcessed = item.lastOffsetProcessed ? { S: item.lastOffsetProcessed } : null;
+  const offsetValue = item.lastOffsetProcessed ? { S: JSON.stringify(item.lastOffsetProcessed) } : null;
   const params = {
     TableName: config.OFFSETS_TABLE_NAME,
     Item: {
       id: { S: item.id },
-      lastOffsetProcessed
+      lastOffsetProcessed: offsetValue
     }
   };
   await writeToTable(item, params);
-  logInfo(`Successfully wrote offset ${JSON.stringify(item.lastOffsetProcessed)} to DynamoDB table ${config.OFFSETS_TABLE_NAME}`); // : ${JSON.stringify(item)}
+  logInfo(`Successfully wrote offset ${JSON.stringify(item.lastOffsetProcessed)} to DynamoDB table ${config.OFFSETS_TABLE_NAME}`);
 }
 
 export async function readLastOffsetProcessedFromOffsetsTableById(id) {
@@ -198,7 +145,13 @@ export async function readLastOffsetProcessedFromOffsetsTableById(id) {
     logInfo(`Got item with id "${id}" from table ${config.OFFSETS_TABLE_NAME}: ${JSON.stringify(result.Item)}.`);
   }
 
-  return result.Item.lastOffsetProcessed === undefined ? undefined : result.Item.lastOffsetProcessed.S;
+  if (!result.Item.lastOffsetProcessed || !result.Item.lastOffsetProcessed.S) return undefined;
+  try {
+    return JSON.parse(result.Item.lastOffsetProcessed.S);
+  } catch (e) {
+    logError(`Failed to parse offset for id ${id}`, e);
+    return undefined;
+  }
 }
 
 export async function writeValueToProjectionsTable(item) {
@@ -211,7 +164,7 @@ export async function writeValueToProjectionsTable(item) {
     }
   };
   await writeToTable(item, params);
-  logInfo(`Successfully wrote value ${JSON.stringify(item.value)} to DynamoDB table ${config.PROJECTIONS_TABLE_NAME}`); // : ${JSON.stringify(item)}
+  logInfo(`Successfully wrote value ${JSON.stringify(item.value)} to DynamoDB table ${config.PROJECTIONS_TABLE_NAME}`);
 }
 
 export async function writeToTable(item, params) {
@@ -292,13 +245,13 @@ export function streamToString(stream) {
 }
 
 export async function getS3ObjectWithContentAndVersion(s3BucketName, key, versionId) {
-  const version = await getS3ObjectVersion(s3BucketName, key, versionId)
-  const { objectMetaData, object } = await getS3ObjectWithContent(s3BucketName, key, versionId)
+  const version = await getS3ObjectVersion(s3BucketName, key, versionId);
+  const { objectMetaData, object } = await getS3ObjectWithContent(s3BucketName, key, versionId);
   return { objectMetaData, object, version };
 }
 
 export async function getS3ObjectWithContent(s3BucketName, key, versionId) {
-  const objectMetaData = await getS3ObjectMetadata(s3BucketName, key, versionId)
+  const objectMetaData = await getS3ObjectMetadata(s3BucketName, key, versionId);
   const object = await streamToString(objectMetaData.Body);
   return { objectMetaData, object };
 }
@@ -308,7 +261,7 @@ export async function getS3ObjectMetadata(s3BucketName, key, versionId) {
     Bucket: s3BucketName,
     Key: key,
     VersionId: versionId
-  }
+  };
   const objectMetaData = await s3.send(new GetObjectCommand(params));
   return objectMetaData;
 }
@@ -318,7 +271,7 @@ export async function getS3ObjectVersion(s3BucketName, key, versionId) {
     Bucket: s3BucketName,
     Key: key,
     VersionId: versionId
-  }
+  };
   const objectMetaData = await s3.send(new GetObjectCommand(params));
   return objectMetaData;
 }
@@ -344,8 +297,6 @@ export async function getProjectionIdsMap(ignoreKeys) {
 
     if (result && result.Items) {
       for (const item of result.Items) {
-        // If you're using the low-level DynamoDB API, attributes might be in the form { S: 'value' }.
-        // Here we assume that a transformation (for example, using DynamoDB DocumentClient) already yields plain values.
         const id = item.id.S;
         if(ignoreKeys && ignoreKeys.includes(id)) {
           continue;
@@ -403,6 +354,7 @@ export function logError(message, error) {
 
 export async function replay() {
   logInfo(`Starting replay job for bucket ${config.BUCKET_NAME} prefix ${config.OBJECT_PREFIX}`);
+  // Initialize offsets with a null offset record
   await writeLastOffsetProcessedToOffsetsTable({
     id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
     lastOffsetProcessed: null
@@ -417,14 +369,14 @@ export async function replay() {
   let eventsReplayed = 0;
   if (versions.length === 0) {
     logInfo('No versions found to process.');
-    lastOffsetProcessed = `${new Date().toISOString()} No versions found to replay`;
+    const offsetRecord = { timestamp: new Date().toISOString(), note: 'No versions found to replay' };
     await writeLastOffsetProcessedToOffsetsTable({
       id: config.REPLAY_QUEUE_URL,
-      lastOffsetProcessed
+      lastOffsetProcessed: offsetRecord
     });
     await writeLastOffsetProcessedToOffsetsTable({
       id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
-      lastOffsetProcessed
+      lastOffsetProcessed: offsetRecord
     });
   } else {
     for (const version of versions) {
@@ -432,7 +384,7 @@ export async function replay() {
       const id = version.Key;
       const versionId = version.VersionId;
       const objectMetaData = await getS3ObjectMetadata(config.BUCKET_NAME, id, versionId);
-      const lastModified = ((objectMetaData === undefined || objectMetaData.LastModified === undefined) ? undefined : objectMetaData.LastModified.toISOString());
+      const lastModified = (objectMetaData && objectMetaData.LastModified) ? objectMetaData.LastModified.toISOString() : undefined;
       const versionMetadata = {
         key: id,
         versionId: versionId,
@@ -443,10 +395,11 @@ export async function replay() {
 
       await sendToSqs(s3Event, config.REPLAY_QUEUE_URL);
 
-      lastOffsetProcessed = `${lastModified} ${id} ${versionId}`;
+      const offsetRecord = { timestamp: lastModified, key: id, versionId };
+      lastOffsetProcessed = offsetRecord;
       await writeLastOffsetProcessedToOffsetsTable({
         id: config.REPLAY_QUEUE_URL,
-        lastOffsetProcessed
+        lastOffsetProcessed: offsetRecord
       });
 
       eventsReplayed++;
@@ -475,34 +428,31 @@ export async function createProjections(s3Event) {
 
 export async function createProjection(s3PutEventRecord) {
   const id = s3PutEventRecord.s3.object.key;
-  const versionId = s3PutEventRecord.s3.object.versionId
+  const versionId = s3PutEventRecord.s3.object.versionId;
   const {objectMetaData, object, version} = await getS3ObjectWithContentAndVersion(config.BUCKET_NAME, id, versionId);
-  //const object = await s3.send(new GetObjectCommand(params));
-  //logInfo(`versionId is: ${JSON.stringify(version.versionId)} for actual object ${object} expected ${versionId}`);
   if (version && !version.IsLatest) {
     logError(`This is not the latest version of the object: ${id} ${versionId}`);
-    // TODO: Add the version to the projection and check if we are older than that (rather than the latest as above)
-    // TODO: Add a count of the number of versions to the projection.
-  } //else {
+    // TODO: Handle non-latest version appropriately
+  }
   await writeValueToProjectionsTable({
     id,
     value: object
   });
-  //}
   const digest = await computeDigest(["digest"]);
   await writeValueToProjectionsTable({
     id: "digest",
     value: JSON.stringify(digest)
   });
-  const lastOffsetProcessed = `${objectMetaData.LastModified.toISOString()} ${id} ${versionId}`
+  
+  const newOffset = { timestamp: objectMetaData.LastModified.toISOString(), key: id, versionId };
   const bucketLastOffsetProcessed = await readLastOffsetProcessedFromOffsetsTableById(`${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`);
-  if (lastOffsetProcessed < bucketLastOffsetProcessed) {
-    logError(`Bucket offset ${bucketLastOffsetProcessed} is already at or ahead of this object's offset at ${lastOffsetProcessed}. Skipping offset update.`);
-  }else{
-    logInfo(`Bucket offset ${bucketLastOffsetProcessed} is being replaced by this object's offset at ${lastOffsetProcessed}.`);
+  if (bucketLastOffsetProcessed && bucketLastOffsetProcessed.timestamp && (new Date(newOffset.timestamp) < new Date(bucketLastOffsetProcessed.timestamp))) {
+    logError(`Bucket offset ${bucketLastOffsetProcessed.timestamp} is already at or ahead of this object's offset at ${newOffset.timestamp}. Skipping offset update.`);
+  } else {
+    logInfo(`Bucket offset ${bucketLastOffsetProcessed ? bucketLastOffsetProcessed.timestamp : 'undefined'} is being replaced by this object's offset at ${newOffset.timestamp}.`);
     await writeLastOffsetProcessedToOffsetsTable({
       id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
-      lastOffsetProcessed
+      lastOffsetProcessed: newOffset
     });
   }
 
@@ -531,19 +481,15 @@ export async function sourceLambdaHandler(sqsEvent) {
     `Source Lambda received event: ${JSON.stringify(sqsEvent)}`
   );
 
-  // If the latest bucket offset processed is null or behind the latest queue offset processed, error out, replay needed.
   const replayQueueLastOffsetProcessed = await readLastOffsetProcessedFromOffsetsTableById(config.REPLAY_QUEUE_URL);
   const bucketLastOffsetProcessed = await readLastOffsetProcessedFromOffsetsTableById(`${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`);
-  if (!bucketLastOffsetProcessed || bucketLastOffsetProcessed < replayQueueLastOffsetProcessed) {
-    throw new Error(`Bucket offset processed ${bucketLastOffsetProcessed} is behind replay queue offset processed ${replayQueueLastOffsetProcessed}. Replay needed.`);
-  }else{
-    logInfo(`Bucket offset processed ${bucketLastOffsetProcessed} is at or ahead of the replay queue offset processed ${replayQueueLastOffsetProcessed}. Ready to read from source.`);
+  if (!bucketLastOffsetProcessed || !bucketLastOffsetProcessed.timestamp || (replayQueueLastOffsetProcessed && new Date(bucketLastOffsetProcessed.timestamp) < new Date(replayQueueLastOffsetProcessed.timestamp))) {
+    throw new Error(`Bucket offset processed ${bucketLastOffsetProcessed ? bucketLastOffsetProcessed.timestamp : 'undefined'} is behind replay queue offset processed ${replayQueueLastOffsetProcessed ? replayQueueLastOffsetProcessed.timestamp : 'undefined'}. Replay needed.`);
+  } else {
+    logInfo(`Bucket offset processed ${bucketLastOffsetProcessed.timestamp} is at or ahead of the replay queue offset processed ${replayQueueLastOffsetProcessed ? replayQueueLastOffsetProcessed.timestamp : 'undefined'}. Ready to read from source.`);
   }
 
-  // If event.Records is an array, use it. Otherwise, treat the event itself as one record.
   const sqsEventRecords = Array.isArray(sqsEvent.Records) ? sqsEvent.Records : [sqsEvent];
-
-  // Array to collect the identifiers of the failed records
   const batchItemFailures = [];
 
   for (const sqsEventRecord of sqsEventRecords) {
@@ -551,9 +497,8 @@ export async function sourceLambdaHandler(sqsEvent) {
       const s3Event = JSON.parse(sqsEventRecord.body);
       const digest = await createProjections(s3Event);
       await sendToSqs(digest, config.DIGEST_QUEUE_URL);
-      logInfo(`Created source-projection for with digest (and TODO dispatched to SQS): ${JSON.stringify(digest)}`);
+      logInfo(`Created source-projection with digest: ${JSON.stringify(digest)}`);
     } catch (error) {
-      // Log the error and add the record's messageId to the partial batch response
       logError(
         `Error processing record ${sqsEventRecord.messageId}: ${error.message}`,
         error
@@ -562,7 +507,6 @@ export async function sourceLambdaHandler(sqsEvent) {
     }
   }
 
-  // Return the list of failed messages so that AWS SQS can attempt to reprocess them.
   return {
     batchItemFailures,
     handler: "src/lib/main.sourceLambdaHandler",
@@ -571,22 +515,15 @@ export async function sourceLambdaHandler(sqsEvent) {
 
 export async function replayLambdaHandler(sqsEvent) {
   logInfo(`Replay Lambda received event: ${JSON.stringify(sqsEvent)}`);
-
-  // If event.Records is an array, use it.
-  // Otherwise, treat the event itself as one record.
   const sqsEventRecords = Array.isArray(sqsEvent.Records) ? sqsEvent.Records : [sqsEvent];
-
-  // Array to collect identifiers for records that failed processing
   const batchItemFailures = [];
 
   for (const sqsEventRecord of sqsEventRecords) {
     try {
       const s3Event = JSON.parse(sqsEventRecord.body);
       const digest = await createProjections(s3Event);
-      // NOTE: Replay does not send the digest via SQS.
       logInfo(`Created replay-projection with digest: ${JSON.stringify(digest)}`);
     } catch (error) {
-      // Log the error and add the record's messageId to the partial batch response
       logError(`Error processing record ${sqsEventRecord.messageId}: ${error.message}`, error);
       batchItemFailures.push({ itemIdentifier: sqsEventRecord.messageId });
     }
@@ -617,7 +554,7 @@ export async function main(args = process.argv.slice(2)) {
     key: 'events/1.json',
     versionId: 'AZW7UcKuQ.8ZZ5GnL9TaTMnK10xH1DON',
     lastModified: new Date().toISOString()
-  }
+  };
   if (args.includes('--help')) {
     console.log(`
       Usage:
@@ -655,4 +592,3 @@ if (import.meta.url.endsWith(process.argv[1])) {
     process.exit(1);
   });
 }
-

commit 55292072e979aeb5f0dc934c0e6c1dbdaa9c58f3
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Tue Mar 25 18:16:10 2025 +0000

    Export utility functions and expand Maven workflows
    
    Exported previously internal utility functions for broader use, addressing key modularity and reusability needs. Enhanced CI/CD workflows by integrating improved Maven packaging and deployment processes for streamlined releases. Updated documentation and added role-assuming scripts to support deployment tasks.

diff --git a/src/lib/main.js b/src/lib/main.js
index f9fef5a..d0a6a05 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -34,9 +34,9 @@ const configSchema = z.object({
   AWS_ENDPOINT: z.string().optional()
 });
 
-const config = configSchema.parse(process.env);
+export const config = configSchema.parse(process.env);
 
-function logConfig() {
+export function logConfig() {
   console.log(JSON.stringify({
     level: "info",
     timestamp: new Date().toISOString(),
@@ -55,10 +55,10 @@ function logConfig() {
 }
 logConfig();
 
-const s3 = new S3Client({ endpoint: config.AWS_ENDPOINT, forcePathStyle: true });
-const sqs = new SQSClient({ endpoint: config.AWS_ENDPOINT });
-const dynamodb = new DynamoDBClient({ endpoint: config.AWS_ENDPOINT });
-const lambda = new LambdaClient();
+export const s3 = new S3Client({ endpoint: config.AWS_ENDPOINT, forcePathStyle: true });
+export const sqs = new SQSClient({ endpoint: config.AWS_ENDPOINT });
+export const dynamodb = new DynamoDBClient({ endpoint: config.AWS_ENDPOINT });
+export const lambda = new LambdaClient();
 
 // ---------------------------------------------------------------------------------------------------------------------
 // AWS Utility functions
@@ -276,7 +276,7 @@ export function createSQSEventFromS3Event(s3Event) {
   };
 }
 
-function streamToString(stream) {
+export function streamToString(stream) {
   return new Promise(function(resolve, reject) {
     const chunks = [];
     stream.on("data", function(chunk) {
@@ -291,19 +291,19 @@ function streamToString(stream) {
   });
 }
 
-async function getS3ObjectWithContentAndVersion(s3BucketName, key, versionId) {
+export async function getS3ObjectWithContentAndVersion(s3BucketName, key, versionId) {
   const version = await getS3ObjectVersion(s3BucketName, key, versionId)
   const { objectMetaData, object } = await getS3ObjectWithContent(s3BucketName, key, versionId)
   return { objectMetaData, object, version };
 }
 
-async function getS3ObjectWithContent(s3BucketName, key, versionId) {
+export async function getS3ObjectWithContent(s3BucketName, key, versionId) {
   const objectMetaData = await getS3ObjectMetadata(s3BucketName, key, versionId)
   const object = await streamToString(objectMetaData.Body);
   return { objectMetaData, object };
 }
 
-async function getS3ObjectMetadata(s3BucketName, key, versionId) {
+export async function getS3ObjectMetadata(s3BucketName, key, versionId) {
   const params = {
     Bucket: s3BucketName,
     Key: key,
@@ -313,7 +313,7 @@ async function getS3ObjectMetadata(s3BucketName, key, versionId) {
   return objectMetaData;
 }
 
-async function getS3ObjectVersion(s3BucketName, key, versionId) {
+export async function getS3ObjectVersion(s3BucketName, key, versionId) {
   const params = {
     Bucket: s3BucketName,
     Key: key,
@@ -360,7 +360,7 @@ export async function getProjectionIdsMap(ignoreKeys) {
   return idsMap;
 }
 
-async function enableDisableEventSourceMapping(functionName, enable) {
+export async function enableDisableEventSourceMapping(functionName, enable) {
     const listMappingsCommand = new ListEventSourceMappingsCommand({
       FunctionName: functionName,
     });
@@ -387,13 +387,13 @@ async function enableDisableEventSourceMapping(functionName, enable) {
 // Utility functions
 // ---------------------------------------------------------------------------------------------------------------------
 
-const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));
+export const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));
 
-function logInfo(message) {
+export function logInfo(message) {
   console.log(JSON.stringify({ level: "info", timestamp: new Date().toISOString(), message }));
 }
 
-function logError(message, error) {
+export function logError(message, error) {
   console.error(JSON.stringify({ level: "error", timestamp: new Date().toISOString(), message, error: error ? error.toString() : undefined }));
 }
 
@@ -602,7 +602,7 @@ export async function replayLambdaHandler(sqsEvent) {
 // Health check server
 // ---------------------------------------------------------------------------------------------------------------------
 
-function healthCheckServer() {
+export function healthCheckServer() {
   const app = express();
   app.get('/', (req, res) => res.send('S3 SQS Bridge OK'));
   app.listen(8080, () => logInfo('Healthcheck available at :8080'));

commit dff7405917d93b692b40d2b3f78c55d008851080
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Mon Mar 24 23:19:13 2025 +0000

    Remove legacy comments and expand project documentation
    
    Removed outdated comments from `computeDigest` function to clean up the codebase. Improved documentation by adding additional project details, tasks, and guidance for deployment and debugging to support developers.

diff --git a/src/lib/main.js b/src/lib/main.js
index eceb692..f9fef5a 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -511,10 +511,6 @@ export async function createProjection(s3PutEventRecord) {
 
 export async function computeDigest(ignoreKeys) {
   const digest = await getProjectionIdsMap(ignoreKeys);
-  // TODO: When we have gathered a sample of events, compute the digests.
-  // TODO: Find a way to externalise the digest so a consuming library can inject a custom digest into the stack.
-  // TODO: Export every useful function here and write some initialisers for re-use too
-  // TODO: Create a sample skeleton implementation that delegates to this library.
   return digest;
 }
 

commit 24aab1e37611a38b0521571bf1d781a0cc9dcecd
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Mon Mar 24 01:28:22 2025 +0000

    Refactor SQS message handling with customizable queue URL.
    
    Replaced hardcoded QueueUrl reference with a parameter to enhance flexibility in SQS message handling. Updated instances of `buildSQSMessageParams` to include the new `sqsQueueUrl` argument. Minor cleanup in logging and improved README example output for clarity.

diff --git a/src/lib/main.js b/src/lib/main.js
index d7b1f76..eceb692 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -147,15 +147,15 @@ export async function listAllObjectVersionsOldestFirst() {
   return merged;
 }
 
-export function buildSQSMessageParams(body) {
+export function buildSQSMessageParams(body, sqsQueueUrl) {
   return {
-    QueueUrl: config.REPLAY_QUEUE_URL,
+    QueueUrl: sqsQueueUrl,
     MessageBody: JSON.stringify(body)
   };
 }
 
 export async function sendToSqs(body, sqsQueueUrl) {
-  const params = buildSQSMessageParams(body);
+  const params = buildSQSMessageParams(body, sqsQueueUrl);
   try {
     const result = await retryOperationExponential(async () =>
         await sqs.send(new SendMessageCommand(params))
@@ -478,8 +478,8 @@ export async function createProjection(s3PutEventRecord) {
   const versionId = s3PutEventRecord.s3.object.versionId
   const {objectMetaData, object, version} = await getS3ObjectWithContentAndVersion(config.BUCKET_NAME, id, versionId);
   //const object = await s3.send(new GetObjectCommand(params));
-  logInfo(`Version object is: ${JSON.stringify(version)} for actual object ${object}...`);
-  if (!version.IsLatest) {
+  //logInfo(`versionId is: ${JSON.stringify(version.versionId)} for actual object ${object} expected ${versionId}`);
+  if (version && !version.IsLatest) {
     logError(`This is not the latest version of the object: ${id} ${versionId}`);
     // TODO: Add the version to the projection and check if we are older than that (rather than the latest as above)
     // TODO: Add a count of the number of versions to the projection.

commit a417bbebab72fe175b5a0431f29eda60dd2e6e0d
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Mon Mar 24 01:12:08 2025 +0000

    Refactor logic and enhance policy handling for S3 and SQS.
    
    Updated `lastOffsetProcessed` handling to account for undefined values and improved logging for object version information. Refactored S

diff --git a/src/lib/main.js b/src/lib/main.js
index 666df16..d7b1f76 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -198,7 +198,7 @@ export async function readLastOffsetProcessedFromOffsetsTableById(id) {
     logInfo(`Got item with id "${id}" from table ${config.OFFSETS_TABLE_NAME}: ${JSON.stringify(result.Item)}.`);
   }
 
-  return result.Item.lastOffsetProcessed.S;
+  return result.Item.lastOffsetProcessed === undefined ? undefined : result.Item.lastOffsetProcessed.S;
 }
 
 export async function writeValueToProjectionsTable(item) {
@@ -478,17 +478,17 @@ export async function createProjection(s3PutEventRecord) {
   const versionId = s3PutEventRecord.s3.object.versionId
   const {objectMetaData, object, version} = await getS3ObjectWithContentAndVersion(config.BUCKET_NAME, id, versionId);
   //const object = await s3.send(new GetObjectCommand(params));
-  logInfo(`Version object is: ${version} for actual object ${object}...`);
+  logInfo(`Version object is: ${JSON.stringify(version)} for actual object ${object}...`);
   if (!version.IsLatest) {
     logError(`This is not the latest version of the object: ${id} ${versionId}`);
     // TODO: Add the version to the projection and check if we are older than that (rather than the latest as above)
     // TODO: Add a count of the number of versions to the projection.
-  } else {
-    await writeValueToProjectionsTable({
-      id,
-      value: object
-    });
-  }
+  } //else {
+  await writeValueToProjectionsTable({
+    id,
+    value: object
+  });
+  //}
   const digest = await computeDigest(["digest"]);
   await writeValueToProjectionsTable({
     id: "digest",
@@ -513,6 +513,8 @@ export async function computeDigest(ignoreKeys) {
   const digest = await getProjectionIdsMap(ignoreKeys);
   // TODO: When we have gathered a sample of events, compute the digests.
   // TODO: Find a way to externalise the digest so a consuming library can inject a custom digest into the stack.
+  // TODO: Export every useful function here and write some initialisers for re-use too
+  // TODO: Create a sample skeleton implementation that delegates to this library.
   return digest;
 }
 

commit ed730ec394a8ab3fc9237b77c556e20578f4c9cd
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Mon Mar 24 00:52:28 2025 +0000

    Improve resilience and testing in S3-SQS bridge logic
    
    Enhance null/undefined checks in main.js to prevent potential runtime errors. Add extensive unit tests covering S3/SQS event creation, DynamoDB scanning, and edge cases in lambda handlers to ensure robust functionality. Update mock implementations and introduce skipped tests for scenarios requiring external dependencies or advanced setups.

diff --git a/src/lib/main.js b/src/lib/main.js
index 9739095..666df16 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -13,7 +13,7 @@ import { LambdaClient, ListEventSourceMappingsCommand, UpdateEventSourceMappingC
 dotenv.config();
 
 if (process.env.VITEST || process.env.NODE_ENV === "development") {
-  process.env.BUCKET_NAME = process.env.BUCKET_NAME || " s3-sqs-bridge-bucket-test";
+  process.env.BUCKET_NAME = process.env.BUCKET_NAME || "s3-sqs-bridge-bucket-test";
   process.env.OBJECT_PREFIX = process.env.OBJECT_PREFIX || "events/";
   process.env.REPLAY_QUEUE_URL = process.env.REPLAY_QUEUE_URL || "http://test/000000000000/s3-sqs-bridge-replay-queue-test";
   process.env.DIGEST_QUEUE_URL = process.env.DIGEST_QUEUE_URL || "http://test/000000000000/s3-sqs-bridge-digest-queue-test";
@@ -342,7 +342,7 @@ export async function getProjectionIdsMap(ignoreKeys) {
 
     const result = await dynamodb.send(new ScanCommand(params));
 
-    if (result.Items) {
+    if (result && result.Items) {
       for (const item of result.Items) {
         // If you're using the low-level DynamoDB API, attributes might be in the form { S: 'value' }.
         // Here we assume that a transformation (for example, using DynamoDB DocumentClient) already yields plain values.
@@ -354,7 +354,7 @@ export async function getProjectionIdsMap(ignoreKeys) {
       }
     }
 
-    lastEvaluatedKey = result.LastEvaluatedKey;
+    lastEvaluatedKey = (result === undefined ? undefined : result.LastEvaluatedKey);
   } while (lastEvaluatedKey);
 
   return idsMap;
@@ -432,7 +432,7 @@ export async function replay() {
       const id = version.Key;
       const versionId = version.VersionId;
       const objectMetaData = await getS3ObjectMetadata(config.BUCKET_NAME, id, versionId);
-      const lastModified = objectMetaData.LastModified.toISOString()
+      const lastModified = ((objectMetaData === undefined || objectMetaData.LastModified === undefined) ? undefined : objectMetaData.LastModified.toISOString());
       const versionMetadata = {
         key: id,
         versionId: versionId,

commit 338617f1237ea39074d48646b99d286794e1a63b
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Mon Mar 24 00:27:08 2025 +0000

    Refactor S3 object handling and adjust SQS queue configurations
    
    Added functions to handle extended S3 object logic, including fetching object versions and metadata. Enhanced replay logic to better handle offsets and avoid redundant updates. Adjusted SQS configurations to improve timeout management and increase max receive count for dead-letter queues.

diff --git a/src/lib/main.js b/src/lib/main.js
index f4e68a1..9739095 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -291,6 +291,12 @@ function streamToString(stream) {
   });
 }
 
+async function getS3ObjectWithContentAndVersion(s3BucketName, key, versionId) {
+  const version = await getS3ObjectVersion(s3BucketName, key, versionId)
+  const { objectMetaData, object } = await getS3ObjectWithContent(s3BucketName, key, versionId)
+  return { objectMetaData, object, version };
+}
+
 async function getS3ObjectWithContent(s3BucketName, key, versionId) {
   const objectMetaData = await getS3ObjectMetadata(s3BucketName, key, versionId)
   const object = await streamToString(objectMetaData.Body);
@@ -307,6 +313,15 @@ async function getS3ObjectMetadata(s3BucketName, key, versionId) {
   return objectMetaData;
 }
 
+async function getS3ObjectVersion(s3BucketName, key, versionId) {
+  const params = {
+    Bucket: s3BucketName,
+    Key: key,
+    VersionId: versionId
+  }
+  const objectMetaData = await s3.send(new GetObjectCommand(params));
+  return objectMetaData;
+}
 
 // Function to scan through all pages in the projections table and return a map:
 // { <id>: { id: <id> } } for each projection.
@@ -413,19 +428,22 @@ export async function replay() {
     });
   } else {
     for (const version of versions) {
+      logInfo(`Replaying version: ${JSON.stringify(version)}`);
       const id = version.Key;
       const versionId = version.VersionId;
       const objectMetaData = await getS3ObjectMetadata(config.BUCKET_NAME, id, versionId);
-      const s3Event = createS3EventFromVersion({
-        key: objectMetaData.Key,
-        versionId: objectMetaData.VersionId,
-        lastModified: objectMetaData.LastModified
-      });
+      const lastModified = objectMetaData.LastModified.toISOString()
+      const versionMetadata = {
+        key: id,
+        versionId: versionId,
+        lastModified
+      };
+      logInfo(`Creating s3 Put event from versionMetadata: ${JSON.stringify(versionMetadata)} from object metadata LastModified: ${objectMetaData.LastModified}`);
+      const s3Event = createS3EventFromVersion(versionMetadata);
 
       await sendToSqs(s3Event, config.REPLAY_QUEUE_URL);
 
-
-      lastOffsetProcessed = `${objectMetaData.LastModified.toISOString()} ${objectMetaData.Key} ${objectMetaData.VersionId}`;
+      lastOffsetProcessed = `${lastModified} ${id} ${versionId}`;
       await writeLastOffsetProcessedToOffsetsTable({
         id: config.REPLAY_QUEUE_URL,
         lastOffsetProcessed
@@ -458,23 +476,36 @@ export async function createProjections(s3Event) {
 export async function createProjection(s3PutEventRecord) {
   const id = s3PutEventRecord.s3.object.key;
   const versionId = s3PutEventRecord.s3.object.versionId
-  const {objectMetaData, object} = await getS3ObjectWithContent(config.BUCKET_NAME, id, versionId);
+  const {objectMetaData, object, version} = await getS3ObjectWithContentAndVersion(config.BUCKET_NAME, id, versionId);
   //const object = await s3.send(new GetObjectCommand(params));
-  logInfo(`Version object is: ${object}...`);
-  await writeValueToProjectionsTable({
-    id,
-    value: object
-  });
+  logInfo(`Version object is: ${version} for actual object ${object}...`);
+  if (!version.IsLatest) {
+    logError(`This is not the latest version of the object: ${id} ${versionId}`);
+    // TODO: Add the version to the projection and check if we are older than that (rather than the latest as above)
+    // TODO: Add a count of the number of versions to the projection.
+  } else {
+    await writeValueToProjectionsTable({
+      id,
+      value: object
+    });
+  }
   const digest = await computeDigest(["digest"]);
   await writeValueToProjectionsTable({
     id: "digest",
     value: JSON.stringify(digest)
   });
   const lastOffsetProcessed = `${objectMetaData.LastModified.toISOString()} ${id} ${versionId}`
-  await writeLastOffsetProcessedToOffsetsTable({
-    id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
-    lastOffsetProcessed
-  });
+  const bucketLastOffsetProcessed = await readLastOffsetProcessedFromOffsetsTableById(`${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`);
+  if (lastOffsetProcessed < bucketLastOffsetProcessed) {
+    logError(`Bucket offset ${bucketLastOffsetProcessed} is already at or ahead of this object's offset at ${lastOffsetProcessed}. Skipping offset update.`);
+  }else{
+    logInfo(`Bucket offset ${bucketLastOffsetProcessed} is being replaced by this object's offset at ${lastOffsetProcessed}.`);
+    await writeLastOffsetProcessedToOffsetsTable({
+      id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
+      lastOffsetProcessed
+    });
+  }
+
   return digest;
 }
 
@@ -555,7 +586,7 @@ export async function replayLambdaHandler(sqsEvent) {
       const s3Event = JSON.parse(sqsEventRecord.body);
       const digest = await createProjections(s3Event);
       // NOTE: Replay does not send the digest via SQS.
-      logInfo(`Created replay-projection with digest: ${digest}`);
+      logInfo(`Created replay-projection with digest: ${JSON.stringify(digest)}`);
     } catch (error) {
       // Log the error and add the record's messageId to the partial batch response
       logError(`Error processing record ${sqsEventRecord.messageId}: ${error.message}`, error);

commit fe4c4e10cfbd27ea831a6ba150bc74396e716ab8
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Sun Mar 23 22:50:46 2025 +0000

    Refactor offset logic and adjust Lambda timeouts.
    
    Removed commented-out code related to offset processing and standardized `LastModified` formatting to use ISO strings. Increased Lambda timeout durations from 2 seconds to 5 seconds to accommodate longer processing times.

diff --git a/src/lib/main.js b/src/lib/main.js
index ce0cc3d..f4e68a1 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -398,15 +398,7 @@ export async function replay() {
   });
   const versions = await listAllObjectVersionsOldestFirst();
   logInfo(`Processing ${versions.length} versions...`);
-  //const latestVersion = versions[versions.length - 1];
   let lastOffsetProcessed = null;
-  //if (latestVersion) {
-  //  lastOffsetProcessed = `${latestVersion.LastModified} ${latestVersion.Key} ${latestVersion.VersionId}`;
-  //}
-  //await writeToOffsetsTable({
-  //  id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
-  //  lastOffsetProcessed
-  //});
   let eventsReplayed = 0;
   if (versions.length === 0) {
     logInfo('No versions found to process.');
@@ -433,7 +425,7 @@ export async function replay() {
       await sendToSqs(s3Event, config.REPLAY_QUEUE_URL);
 
 
-      lastOffsetProcessed = `${objectMetaData.LastModified} ${objectMetaData.Key} ${objectMetaData.VersionId}`;
+      lastOffsetProcessed = `${objectMetaData.LastModified.toISOString()} ${objectMetaData.Key} ${objectMetaData.VersionId}`;
       await writeLastOffsetProcessedToOffsetsTable({
         id: config.REPLAY_QUEUE_URL,
         lastOffsetProcessed
@@ -478,9 +470,10 @@ export async function createProjection(s3PutEventRecord) {
     id: "digest",
     value: JSON.stringify(digest)
   });
+  const lastOffsetProcessed = `${objectMetaData.LastModified.toISOString()} ${id} ${versionId}`
   await writeLastOffsetProcessedToOffsetsTable({
     id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
-    lastOffsetProcessed: `${objectMetaData.LastModified} ${id} ${versionId}`
+    lastOffsetProcessed
   });
   return digest;
 }

commit d1c686e028502cdb0a3ffba2566c4dbdc0ce985b
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Sun Mar 23 22:21:10 2025 +0000

    Refactor variable scope and update log group deletion steps.
    
    Moved `eventsReplayed` variable outside the conditional block to fix scope issues. Updated the README with instructions to delete specific CloudWatch log groups when destroying a stack.

diff --git a/src/lib/main.js b/src/lib/main.js
index 8d684fb..ce0cc3d 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -407,6 +407,7 @@ export async function replay() {
   //  id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
   //  lastOffsetProcessed
   //});
+  let eventsReplayed = 0;
   if (versions.length === 0) {
     logInfo('No versions found to process.');
     lastOffsetProcessed = `${new Date().toISOString()} No versions found to replay`;
@@ -419,7 +420,6 @@ export async function replay() {
       lastOffsetProcessed
     });
   } else {
-    let eventsReplayed = 0;
     for (const version of versions) {
       const id = version.Key;
       const versionId = version.VersionId;

commit c78ecb7ccbae9fa02e3b33146ca2f6277ab4d4b0
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Sun Mar 23 22:00:56 2025 +0000

    Refactor S3 object handling and improve replay logic.
    
    Extracted metadata retrieval into a separate function for clarity and reusability. Added handling for cases when no S3 versions are found, ensuring proper logging and updates to the offsets table. Improved loop logic to streamline processing and enhance readability.

diff --git a/src/lib/main.js b/src/lib/main.js
index b1ac059..8d684fb 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -292,16 +292,22 @@ function streamToString(stream) {
 }
 
 async function getS3ObjectWithContent(s3BucketName, key, versionId) {
+  const objectMetaData = await getS3ObjectMetadata(s3BucketName, key, versionId)
+  const object = await streamToString(objectMetaData.Body);
+  return { objectMetaData, object };
+}
+
+async function getS3ObjectMetadata(s3BucketName, key, versionId) {
   const params = {
     Bucket: s3BucketName,
     Key: key,
     VersionId: versionId
   }
   const objectMetaData = await s3.send(new GetObjectCommand(params));
-  const object = await streamToString(objectMetaData.Body);
-  return { objectMetaData, object };
+  return objectMetaData;
 }
 
+
 // Function to scan through all pages in the projections table and return a map:
 // { <id>: { id: <id> } } for each projection.
 export async function getProjectionIdsMap(ignoreKeys) {
@@ -401,19 +407,40 @@ export async function replay() {
   //  id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
   //  lastOffsetProcessed
   //});
-  let eventsReplayed = 0;
-  for (const version of versions) {
-    const s3Event = createS3EventFromVersion({key: version.Key, versionId: version.VersionId, lastModified: version.LastModified});
-
-    await sendToSqs(s3Event, config.REPLAY_QUEUE_URL);
-
-    lastOffsetProcessed = `${version.LastModified} ${version.Key} ${version.VersionId}`;
+  if (versions.length === 0) {
+    logInfo('No versions found to process.');
+    lastOffsetProcessed = `${new Date().toISOString()} No versions found to replay`;
     await writeLastOffsetProcessedToOffsetsTable({
       id: config.REPLAY_QUEUE_URL,
       lastOffsetProcessed
     });
-
-    eventsReplayed++;
+    await writeLastOffsetProcessedToOffsetsTable({
+      id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
+      lastOffsetProcessed
+    });
+  } else {
+    let eventsReplayed = 0;
+    for (const version of versions) {
+      const id = version.Key;
+      const versionId = version.VersionId;
+      const objectMetaData = await getS3ObjectMetadata(config.BUCKET_NAME, id, versionId);
+      const s3Event = createS3EventFromVersion({
+        key: objectMetaData.Key,
+        versionId: objectMetaData.VersionId,
+        lastModified: objectMetaData.LastModified
+      });
+
+      await sendToSqs(s3Event, config.REPLAY_QUEUE_URL);
+
+
+      lastOffsetProcessed = `${objectMetaData.LastModified} ${objectMetaData.Key} ${objectMetaData.VersionId}`;
+      await writeLastOffsetProcessedToOffsetsTable({
+        id: config.REPLAY_QUEUE_URL,
+        lastOffsetProcessed
+      });
+
+      eventsReplayed++;
+    }
   }
   logInfo('replay job complete.');
   return { versions: versions.length, eventsReplayed, lastOffsetProcessed };

commit 80ea4e115ddcc4d1c16fd7ab4dc2dedde8ba9b47
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Sun Mar 23 21:36:26 2025 +0000

    Refactor DynamoDB operations and enhance logging.
    
    Renamed functions for more clarity, streamlined DynamoDB write/read logic, and added detailed logging for key operations. Adjusted AWS CDK stack to include replay queue URL and updated SQS batching configuration. Updated dependencies to include Lambda client and refined `.gitignore` for temporary files.

diff --git a/src/lib/main.js b/src/lib/main.js
index 1cb5fa5..b1ac059 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -167,7 +167,7 @@ export async function sendToSqs(body, sqsQueueUrl) {
   }
 }
 
-export async function writeToOffsetsTable(item) {
+export async function writeLastOffsetProcessedToOffsetsTable(item) {
   const lastOffsetProcessed = item.lastOffsetProcessed ? { S: item.lastOffsetProcessed } : null;
   const params = {
     TableName: config.OFFSETS_TABLE_NAME,
@@ -177,9 +177,10 @@ export async function writeToOffsetsTable(item) {
     }
   };
   await writeToTable(item, params);
+  logInfo(`Successfully wrote offset ${JSON.stringify(item.lastOffsetProcessed)} to DynamoDB table ${config.OFFSETS_TABLE_NAME}`); // : ${JSON.stringify(item)}
 }
 
-export async function readFromOffsetsTable(id) {
+export async function readLastOffsetProcessedFromOffsetsTableById(id) {
   const params = {
     TableName: config.OFFSETS_TABLE_NAME,
     Key: {
@@ -188,16 +189,19 @@ export async function readFromOffsetsTable(id) {
     ConsistentRead: true,
   };
 
+  logInfo(`Getting item with id "${id}" from table ${config.OFFSETS_TABLE_NAME}.`);
   const result = await dynamodb.send(new GetItemCommand(params));
 
   if (!result.Item) {
     throw new Error(`Item with id "${id}" not found in table ${config.OFFSETS_TABLE_NAME}.`);
+  } else {
+    logInfo(`Got item with id "${id}" from table ${config.OFFSETS_TABLE_NAME}: ${JSON.stringify(result.Item)}.`);
   }
 
-  return result.Item;
+  return result.Item.lastOffsetProcessed.S;
 }
 
-export async function writeToProjectionsTable(item) {
+export async function writeValueToProjectionsTable(item) {
   const value = item.value ? { S: item.value } : null;
   const params = {
     TableName: config.PROJECTIONS_TABLE_NAME,
@@ -207,14 +211,14 @@ export async function writeToProjectionsTable(item) {
     }
   };
   await writeToTable(item, params);
+  logInfo(`Successfully wrote value ${JSON.stringify(item.value)} to DynamoDB table ${config.PROJECTIONS_TABLE_NAME}`); // : ${JSON.stringify(item)}
 }
 
 export async function writeToTable(item, params) {
   try {
     await dynamodb.send(new PutItemCommand(params));
-    logInfo(`Successfully wrote offset to DynamoDB table ${config.OFFSETS_TABLE_NAME}`); // : ${JSON.stringify(item)}
   } catch (error) {
-    logError(`Error writing offset to DynamoDB table ${config.OFFSETS_TABLE_NAME}`, error);
+    logError(`Error writing offset to DynamoDB table ${params.TableName}`, error);
     throw error;
   }
 }
@@ -321,7 +325,7 @@ export async function getProjectionIdsMap(ignoreKeys) {
       for (const item of result.Items) {
         // If you're using the low-level DynamoDB API, attributes might be in the form { S: 'value' }.
         // Here we assume that a transformation (for example, using DynamoDB DocumentClient) already yields plain values.
-        const id = item.id;
+        const id = item.id.S;
         if(ignoreKeys && ignoreKeys.includes(id)) {
           continue;
         }
@@ -378,11 +382,11 @@ function logError(message, error) {
 
 export async function replay() {
   logInfo(`Starting replay job for bucket ${config.BUCKET_NAME} prefix ${config.OBJECT_PREFIX}`);
-  await writeToOffsetsTable({
+  await writeLastOffsetProcessedToOffsetsTable({
     id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
     lastOffsetProcessed: null
   });
-  await writeToOffsetsTable({
+  await writeLastOffsetProcessedToOffsetsTable({
     id: config.REPLAY_QUEUE_URL,
     lastOffsetProcessed: null
   });
@@ -391,7 +395,7 @@ export async function replay() {
   //const latestVersion = versions[versions.length - 1];
   let lastOffsetProcessed = null;
   //if (latestVersion) {
-  //  lastOffsetProcessed = `${latestVersion.Key} ${latestVersion.LastModified} ${latestVersion.VersionId}`;
+  //  lastOffsetProcessed = `${latestVersion.LastModified} ${latestVersion.Key} ${latestVersion.VersionId}`;
   //}
   //await writeToOffsetsTable({
   //  id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
@@ -403,8 +407,8 @@ export async function replay() {
 
     await sendToSqs(s3Event, config.REPLAY_QUEUE_URL);
 
-    lastOffsetProcessed = `${version.Key} ${version.LastModified} ${version.VersionId}`;
-    await writeToOffsetsTable({
+    lastOffsetProcessed = `${version.LastModified} ${version.Key} ${version.VersionId}`;
+    await writeLastOffsetProcessedToOffsetsTable({
       id: config.REPLAY_QUEUE_URL,
       lastOffsetProcessed
     });
@@ -420,14 +424,16 @@ export async function replay() {
 // ---------------------------------------------------------------------------------------------------------------------
 
 export async function createProjections(s3Event) {
-  logInfo(`Creating projections from: ${JSON.stringify(s3Event, null, 2)}...`);
+  logInfo(`Creating projections from: ${JSON.stringify(s3Event)}...`);
   const s3EventRecords = s3Event.Records || [];
+  let digest = null;
   for (const s3EventRecord of s3EventRecords) {
     if (s3EventRecord.eventName !== 'ObjectCreated:Put') {
       throw new Error(`Unsupported event name: ${s3EventRecord.eventName}`);
     }
-    await createProjection(s3EventRecord);
+    digest = await createProjection(s3EventRecord);
   }
+  return digest;
 }
 
 export async function createProjection(s3PutEventRecord) {
@@ -435,19 +441,19 @@ export async function createProjection(s3PutEventRecord) {
   const versionId = s3PutEventRecord.s3.object.versionId
   const {objectMetaData, object} = await getS3ObjectWithContent(config.BUCKET_NAME, id, versionId);
   //const object = await s3.send(new GetObjectCommand(params));
-  logInfo(`Version object is: ${JSON.stringify(object)}...`);
-  await writeToProjectionsTable({
+  logInfo(`Version object is: ${object}...`);
+  await writeValueToProjectionsTable({
     id,
-    value: object.Body //JSON.stringify(object.Body)
+    value: object
   });
   const digest = await computeDigest(["digest"]);
-  await writeToProjectionsTable({
+  await writeValueToProjectionsTable({
     id: "digest",
-    value: object.Body //JSON.stringify(object.Body)
+    value: JSON.stringify(digest)
   });
-  await writeToOffsetsTable({
+  await writeLastOffsetProcessedToOffsetsTable({
     id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
-    lastOffsetProcessed: `${id} ${objectMetaData.LastModified} ${versionId}`
+    lastOffsetProcessed: `${objectMetaData.LastModified} ${id} ${versionId}`
   });
   return digest;
 }
@@ -456,7 +462,7 @@ export async function computeDigest(ignoreKeys) {
   const digest = await getProjectionIdsMap(ignoreKeys);
   // TODO: When we have gathered a sample of events, compute the digests.
   // TODO: Find a way to externalise the digest so a consuming library can inject a custom digest into the stack.
-  return await digest;
+  return digest;
 }
 
 // ---------------------------------------------------------------------------------------------------------------------
@@ -464,7 +470,7 @@ export async function computeDigest(ignoreKeys) {
 // ---------------------------------------------------------------------------------------------------------------------
 
 export async function replayBatchLambdaHandler(event) {
-  logInfo(`Replay Batch Lambda received event: ${JSON.stringify(event, null, 2)}`);
+  logInfo(`Replay Batch Lambda received event: ${JSON.stringify(event)}`);
   //await enableDisableEventSourceMapping(config.SOURCE_LAMBDA_FUNCTION_NAME, false);
   const { versions, eventsReplayed, lastOffsetProcessed } = await replay();
   //await enableDisableEventSourceMapping(config.SOURCE_LAMBDA_FUNCTION_NAME, true);
@@ -473,14 +479,16 @@ export async function replayBatchLambdaHandler(event) {
 
 export async function sourceLambdaHandler(sqsEvent) {
   logInfo(
-    `Source Lambda received event: ${JSON.stringify(sqsEvent, null, 2)}`
+    `Source Lambda received event: ${JSON.stringify(sqsEvent)}`
   );
 
   // If the latest bucket offset processed is null or behind the latest queue offset processed, error out, replay needed.
-  const { lastOffsetProcessed: replayQueueLastOffsetProcessed } = await readFromOffsetsTable(config.REPLAY_QUEUE_URL);
-  const { lastOffsetProcessed: bucketLastOffsetProcessed } = await readFromOffsetsTable(`${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`);
+  const replayQueueLastOffsetProcessed = await readLastOffsetProcessedFromOffsetsTableById(config.REPLAY_QUEUE_URL);
+  const bucketLastOffsetProcessed = await readLastOffsetProcessedFromOffsetsTableById(`${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`);
   if (!bucketLastOffsetProcessed || bucketLastOffsetProcessed < replayQueueLastOffsetProcessed) {
     throw new Error(`Bucket offset processed ${bucketLastOffsetProcessed} is behind replay queue offset processed ${replayQueueLastOffsetProcessed}. Replay needed.`);
+  }else{
+    logInfo(`Bucket offset processed ${bucketLastOffsetProcessed} is at or ahead of the replay queue offset processed ${replayQueueLastOffsetProcessed}. Ready to read from source.`);
   }
 
   // If event.Records is an array, use it. Otherwise, treat the event itself as one record.
@@ -494,7 +502,7 @@ export async function sourceLambdaHandler(sqsEvent) {
       const s3Event = JSON.parse(sqsEventRecord.body);
       const digest = await createProjections(s3Event);
       await sendToSqs(digest, config.DIGEST_QUEUE_URL);
-      logInfo(`Created source-projection for with digest (and TODO dispatched to SQS): ${digest}`);
+      logInfo(`Created source-projection for with digest (and TODO dispatched to SQS): ${JSON.stringify(digest)}`);
     } catch (error) {
       // Log the error and add the record's messageId to the partial batch response
       logError(
@@ -513,7 +521,7 @@ export async function sourceLambdaHandler(sqsEvent) {
 }
 
 export async function replayLambdaHandler(sqsEvent) {
-  logInfo(`Replay Lambda received event: ${JSON.stringify(sqsEvent, null, 2)}`);
+  logInfo(`Replay Lambda received event: ${JSON.stringify(sqsEvent)}`);
 
   // If event.Records is an array, use it.
   // Otherwise, treat the event itself as one record.
@@ -527,7 +535,7 @@ export async function replayLambdaHandler(sqsEvent) {
       const s3Event = JSON.parse(sqsEventRecord.body);
       const digest = await createProjections(s3Event);
       // NOTE: Replay does not send the digest via SQS.
-      logInfo(`Created replay-projection for with digest: ${digest}`);
+      logInfo(`Created replay-projection with digest: ${digest}`);
     } catch (error) {
       // Log the error and add the record's messageId to the partial batch response
       logError(`Error processing record ${sqsEventRecord.messageId}: ${error.message}`, error);

commit 1a8ccde7cd70fb9549624b4fc7117351bcaaead3
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Sun Mar 23 12:50:44 2025 +0000

    Disable EventSourceMapping updates for replay handling.
    
    Commented out calls to enable/disable event source mapping in `replayBatchLambdaHandler` to prevent unnecessary updates. Updated IAM policy to allow all resources for flexibility, with the previous resource specification commented out for future reference.

diff --git a/src/lib/main.js b/src/lib/main.js
index 1b3c5f9..1cb5fa5 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -465,9 +465,9 @@ export async function computeDigest(ignoreKeys) {
 
 export async function replayBatchLambdaHandler(event) {
   logInfo(`Replay Batch Lambda received event: ${JSON.stringify(event, null, 2)}`);
-  await enableDisableEventSourceMapping(config.SOURCE_LAMBDA_FUNCTION_NAME, false);
+  //await enableDisableEventSourceMapping(config.SOURCE_LAMBDA_FUNCTION_NAME, false);
   const { versions, eventsReplayed, lastOffsetProcessed } = await replay();
-  await enableDisableEventSourceMapping(config.SOURCE_LAMBDA_FUNCTION_NAME, true);
+  //await enableDisableEventSourceMapping(config.SOURCE_LAMBDA_FUNCTION_NAME, true);
   return { handler: "src/lib/main.replayBatchLambdaHandler", versions, eventsReplayed, lastOffsetProcessed };
 }
 

commit af034694912a5d2eeed2a145ed02fd7e93c8762a
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Sun Mar 23 07:26:29 2025 +0000

    Refactor S3-SQS Bridge: enhance functionality and modularity
    
    Refactored main functions to decouple SQS queue operations and support additional AWS components like Lambda. Streamlined DynamoDB operations by adding read methods and simplifying key schemas. Introduced digest queue handling and enhanced replay/source Lambda logic for better error handling and process isolation. Updated README and added relevant tests to ensure reliability.

diff --git a/src/lib/main.js b/src/lib/main.js
index 6bc16f6..1b3c5f9 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -7,7 +7,8 @@ import { z } from 'zod';
 import express from 'express';
 import { S3Client, ListObjectVersionsCommand, GetObjectCommand } from '@aws-sdk/client-s3';
 import { SQSClient, SendMessageCommand } from '@aws-sdk/client-sqs';
-import { DynamoDBClient, PutItemCommand } from '@aws-sdk/client-dynamodb';
+import { DynamoDBClient, GetItemCommand, ScanCommand, PutItemCommand } from '@aws-sdk/client-dynamodb';
+import { LambdaClient, ListEventSourceMappingsCommand, UpdateEventSourceMappingCommand } from "@aws-sdk/client-lambda";
 
 dotenv.config();
 
@@ -15,8 +16,10 @@ if (process.env.VITEST || process.env.NODE_ENV === "development") {
   process.env.BUCKET_NAME = process.env.BUCKET_NAME || " s3-sqs-bridge-bucket-test";
   process.env.OBJECT_PREFIX = process.env.OBJECT_PREFIX || "events/";
   process.env.REPLAY_QUEUE_URL = process.env.REPLAY_QUEUE_URL || "http://test/000000000000/s3-sqs-bridge-replay-queue-test";
+  process.env.DIGEST_QUEUE_URL = process.env.DIGEST_QUEUE_URL || "http://test/000000000000/s3-sqs-bridge-digest-queue-test";
   process.env.OFFSETS_TABLE_NAME = process.env.OFFSETS_TABLE_NAME || "s3-sqs-bridge-offsets-table-test";
   process.env.PROJECTIONS_TABLE_NAME = process.env.PROJECTIONS_TABLE_NAME || "s3-sqs-bridge-projections-table-test";
+  process.env.SOURCE_LAMBDA_FUNCTION_NAME = process.env.SOURCE_LAMBDA_FUNCTION_NAME || "s3-sqs-bridge-source-lambda-test";
   process.env.AWS_ENDPOINT = process.env.AWS_ENDPOINT || "http://test";
 }
 
@@ -24,8 +27,10 @@ const configSchema = z.object({
   BUCKET_NAME: z.string().optional(),
   OBJECT_PREFIX: z.string().optional(),
   REPLAY_QUEUE_URL: z.string().optional(),
+  DIGEST_QUEUE_URL: z.string().optional(),
   OFFSETS_TABLE_NAME: z.string().optional(),
   PROJECTIONS_TABLE_NAME: z.string().optional(),
+  SOURCE_LAMBDA_FUNCTION_NAME: z.string().optional(),
   AWS_ENDPOINT: z.string().optional()
 });
 
@@ -40,8 +45,10 @@ function logConfig() {
       BUCKET_NAME: config.BUCKET_NAME,
       OBJECT_PREFIX: config.OBJECT_PREFIX,
       REPLAY_QUEUE_URL: config.REPLAY_QUEUE_URL,
+      DIGEST_QUEUE_URL: config.DIGEST_QUEUE_URL,
       OFFSETS_TABLE_NAME: config.OFFSETS_TABLE_NAME,
       PROJECTIONS_TABLE_NAME: config.PROJECTIONS_TABLE_NAME,
+      SOURCE_LAMBDA_FUNCTION_NAME: config.SOURCE_LAMBDA_FUNCTION_NAME,
       AWS_ENDPOINT: config.AWS_ENDPOINT
     }
   }));
@@ -50,7 +57,8 @@ logConfig();
 
 const s3 = new S3Client({ endpoint: config.AWS_ENDPOINT, forcePathStyle: true });
 const sqs = new SQSClient({ endpoint: config.AWS_ENDPOINT });
-const dynamoClient = new DynamoDBClient({ endpoint: config.AWS_ENDPOINT });
+const dynamodb = new DynamoDBClient({ endpoint: config.AWS_ENDPOINT });
+const lambda = new LambdaClient();
 
 // ---------------------------------------------------------------------------------------------------------------------
 // AWS Utility functions
@@ -139,22 +147,22 @@ export async function listAllObjectVersionsOldestFirst() {
   return merged;
 }
 
-export function buildSQSMessageParams(event) {
+export function buildSQSMessageParams(body) {
   return {
     QueueUrl: config.REPLAY_QUEUE_URL,
-    MessageBody: JSON.stringify(event)
+    MessageBody: JSON.stringify(body)
   };
 }
 
-export async function sendEventToSqs(event) {
-  const params = buildSQSMessageParams(event);
+export async function sendToSqs(body, sqsQueueUrl) {
+  const params = buildSQSMessageParams(body);
   try {
     const result = await retryOperationExponential(async () =>
         await sqs.send(new SendMessageCommand(params))
     );
-    logInfo(`Sent message to SQS queue ${config.REPLAY_QUEUE_URL}, MessageId: ${result.MessageId}`);
+    logInfo(`Sent message to SQS queue ${sqsQueueUrl}, MessageId: ${result.MessageId}`);
   } catch (err) {
-    logError(`Failed to send message to SQS queue ${config.REPLAY_QUEUE_URL}`, err);
+    logError(`Failed to send message to SQS queue ${sqsQueueUrl}`, err);
     throw err;
   }
 }
@@ -165,20 +173,36 @@ export async function writeToOffsetsTable(item) {
     TableName: config.OFFSETS_TABLE_NAME,
     Item: {
       id: { S: item.id },
-      lastModified: { S: item.lastModified.toString() },
       lastOffsetProcessed
     }
   };
   await writeToTable(item, params);
 }
 
+export async function readFromOffsetsTable(id) {
+  const params = {
+    TableName: config.OFFSETS_TABLE_NAME,
+    Key: {
+      id: { S: id }
+    },
+    ConsistentRead: true,
+  };
+
+  const result = await dynamodb.send(new GetItemCommand(params));
+
+  if (!result.Item) {
+    throw new Error(`Item with id "${id}" not found in table ${config.OFFSETS_TABLE_NAME}.`);
+  }
+
+  return result.Item;
+}
+
 export async function writeToProjectionsTable(item) {
   const value = item.value ? { S: item.value } : null;
   const params = {
     TableName: config.PROJECTIONS_TABLE_NAME,
     Item: {
       id: { S: item.id },
-      lastModified: { S: item.lastModified.toString() },
       value
     }
   };
@@ -187,23 +211,14 @@ export async function writeToProjectionsTable(item) {
 
 export async function writeToTable(item, params) {
   try {
-    await dynamoClient.send(new PutItemCommand(params));
-    logInfo(`Successfully written offset to DynamoDB ${config.OFFSETS_TABLE_NAME}`); // : ${JSON.stringify(item)}
+    await dynamodb.send(new PutItemCommand(params));
+    logInfo(`Successfully wrote offset to DynamoDB table ${config.OFFSETS_TABLE_NAME}`); // : ${JSON.stringify(item)}
   } catch (error) {
-    logError("Error writing offset to DynamoDB", error);
-    // Rethrow the error or handle it as needed
+    logError(`Error writing offset to DynamoDB table ${config.OFFSETS_TABLE_NAME}`, error);
     throw error;
   }
 }
 
-export function parseMessageBody(text) {
-  try {
-    return JSON.parse(text);
-  } catch (e) {
-    return null;
-  }
-}
-
 export async function retryOperationExponential(operation, retries = 3, delay = 100) {
   let attempt = 0;
   while (attempt < retries) {
@@ -272,11 +287,75 @@ function streamToString(stream) {
   });
 }
 
-function getObjectContent(params, s3) {
-  return s3.send(new GetObjectCommand(params))
-    .then(function(object) {
-      return streamToString(object.Body);
+async function getS3ObjectWithContent(s3BucketName, key, versionId) {
+  const params = {
+    Bucket: s3BucketName,
+    Key: key,
+    VersionId: versionId
+  }
+  const objectMetaData = await s3.send(new GetObjectCommand(params));
+  const object = await streamToString(objectMetaData.Body);
+  return { objectMetaData, object };
+}
+
+// Function to scan through all pages in the projections table and return a map:
+// { <id>: { id: <id> } } for each projection.
+export async function getProjectionIdsMap(ignoreKeys) {
+  let lastEvaluatedKey = undefined;
+  const idsMap = {};
+
+  // Continue scanning while there are more pages.
+  do {
+    const params = {
+      TableName: config.PROJECTIONS_TABLE_NAME,
+      ExclusiveStartKey: lastEvaluatedKey,
+      // Use a ProjectionExpression to get only the id attribute.
+      ProjectionExpression: 'id',
+      // Using strong consistency in the read so that we see the most recent write.
+      ConsistentRead: true,
+    };
+
+    const result = await dynamodb.send(new ScanCommand(params));
+
+    if (result.Items) {
+      for (const item of result.Items) {
+        // If you're using the low-level DynamoDB API, attributes might be in the form { S: 'value' }.
+        // Here we assume that a transformation (for example, using DynamoDB DocumentClient) already yields plain values.
+        const id = item.id;
+        if(ignoreKeys && ignoreKeys.includes(id)) {
+          continue;
+        }
+        idsMap[id] = { id };
+      }
+    }
+
+    lastEvaluatedKey = result.LastEvaluatedKey;
+  } while (lastEvaluatedKey);
+
+  return idsMap;
+}
+
+async function enableDisableEventSourceMapping(functionName, enable) {
+    const listMappingsCommand = new ListEventSourceMappingsCommand({
+      FunctionName: functionName,
+    });
+    const mappingsResponse = await lambda.send(listMappingsCommand);
+
+    if (!mappingsResponse.EventSourceMappings || mappingsResponse.EventSourceMappings.length === 0) {
+      throw new Error(`No event source mappings found for function ${functionName}`);
+    }
+
+    const uuid = mappingsResponse.EventSourceMappings[0].UUID;
+    if (!uuid) {
+      throw new Error("Unable to retrieve UUID from event source mapping.");
+    }
+
+    const updateMappingCommand = new UpdateEventSourceMappingCommand({
+      UUID: uuid,
+      Enabled: enable,
     });
+    const updateResponse = await lambda.send(updateMappingCommand);
+    logInfo("Event source mapping disabled:", updateResponse);
 }
 
 // ---------------------------------------------------------------------------------------------------------------------
@@ -299,29 +378,35 @@ function logError(message, error) {
 
 export async function replay() {
   logInfo(`Starting replay job for bucket ${config.BUCKET_NAME} prefix ${config.OBJECT_PREFIX}`);
-  const versions = await listAllObjectVersionsOldestFirst();
-  logInfo(`Processing ${versions.length} versions...`);
-  const latestVersion = versions[versions.length - 1];
-  const now = new Date().toISOString();
-  let lastOffsetProcessed = null;
-  if (latestVersion) {
-    lastOffsetProcessed = `${latestVersion.Key} ${latestVersion.LastModified} ${latestVersion.VersionId}`;
-  }
   await writeToOffsetsTable({
     id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
-    lastModified: now,
-    lastOffsetProcessed
+    lastOffsetProcessed: null
+  });
+  await writeToOffsetsTable({
+    id: config.REPLAY_QUEUE_URL,
+    lastOffsetProcessed: null
   });
+  const versions = await listAllObjectVersionsOldestFirst();
+  logInfo(`Processing ${versions.length} versions...`);
+  //const latestVersion = versions[versions.length - 1];
+  let lastOffsetProcessed = null;
+  //if (latestVersion) {
+  //  lastOffsetProcessed = `${latestVersion.Key} ${latestVersion.LastModified} ${latestVersion.VersionId}`;
+  //}
+  //await writeToOffsetsTable({
+  //  id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
+  //  lastOffsetProcessed
+  //});
   let eventsReplayed = 0;
   for (const version of versions) {
     const s3Event = createS3EventFromVersion({key: version.Key, versionId: version.VersionId, lastModified: version.LastModified});
 
-    await sendEventToSqs(s3Event);
+    await sendToSqs(s3Event, config.REPLAY_QUEUE_URL);
 
+    lastOffsetProcessed = `${version.Key} ${version.LastModified} ${version.VersionId}`;
     await writeToOffsetsTable({
       id: config.REPLAY_QUEUE_URL,
-      lastModified: now,
-      lastOffsetProcessed: `${version.Key} ${version.LastModified} ${version.VersionId}`
+      lastOffsetProcessed
     });
 
     eventsReplayed++;
@@ -338,49 +423,40 @@ export async function createProjections(s3Event) {
   logInfo(`Creating projections from: ${JSON.stringify(s3Event, null, 2)}...`);
   const s3EventRecords = s3Event.Records || [];
   for (const s3EventRecord of s3EventRecords) {
-    if (s3EventRecord.eventName === 'ObjectCreated:Put') {
-      await createProjection(s3EventRecord)
-    } else {
-      logError(`Unsupported event name: ${s3EventRecord.eventName}`);
+    if (s3EventRecord.eventName !== 'ObjectCreated:Put') {
+      throw new Error(`Unsupported event name: ${s3EventRecord.eventName}`);
     }
+    await createProjection(s3EventRecord);
   }
 }
 
 export async function createProjection(s3PutEventRecord) {
-  if (s3PutEventRecord.eventName === 'ObjectCreated:Put') {
-    const id = s3PutEventRecord.s3.object.key;
-    const now = new Date().toISOString();
-    const params = {
-      Bucket: config.BUCKET_NAME,
-      Key: id,
-      VersionId: s3PutEventRecord.s3.object.versionId
-    }
-    const object = await getObjectContent(params, s3);
-    //const object = await s3.send(new GetObjectCommand(params));
-    //logInfo(`Version body is: ${JSON.stringify(object.Body)}...`);
-    await writeToProjectionsTable({
-      id,
-      lastModified: now,
-      value: object.Body //JSON.stringify(object.Body)
-    });
-    await writeToOffsetsTable({
-      id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
-      lastModified: now,
-      lastOffsetProcessed: `${object.Key} ${object.LastModified} ${object.VersionId}`
-    });
-  } else {
-    logError(`Unsupported event name: ${s3PutEventRecord.eventName}`);
-  }
+  const id = s3PutEventRecord.s3.object.key;
+  const versionId = s3PutEventRecord.s3.object.versionId
+  const {objectMetaData, object} = await getS3ObjectWithContent(config.BUCKET_NAME, id, versionId);
+  //const object = await s3.send(new GetObjectCommand(params));
+  logInfo(`Version object is: ${JSON.stringify(object)}...`);
+  await writeToProjectionsTable({
+    id,
+    value: object.Body //JSON.stringify(object.Body)
+  });
+  const digest = await computeDigest(["digest"]);
+  await writeToProjectionsTable({
+    id: "digest",
+    value: object.Body //JSON.stringify(object.Body)
+  });
+  await writeToOffsetsTable({
+    id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
+    lastOffsetProcessed: `${id} ${objectMetaData.LastModified} ${versionId}`
+  });
+  return digest;
 }
 
-// ---------------------------------------------------------------------------------------------------------------------
-// Health check server
-// ---------------------------------------------------------------------------------------------------------------------
-
-function healthCheckServer() {
-  const app = express();
-  app.get('/', (req, res) => res.send('S3 SQS Bridge OK'));
-  app.listen(8080, () => logInfo('Healthcheck available at :8080'));
+export async function computeDigest(ignoreKeys) {
+  const digest = await getProjectionIdsMap(ignoreKeys);
+  // TODO: When we have gathered a sample of events, compute the digests.
+  // TODO: Find a way to externalise the digest so a consuming library can inject a custom digest into the stack.
+  return await digest;
 }
 
 // ---------------------------------------------------------------------------------------------------------------------
@@ -389,28 +465,51 @@ function healthCheckServer() {
 
 export async function replayBatchLambdaHandler(event) {
   logInfo(`Replay Batch Lambda received event: ${JSON.stringify(event, null, 2)}`);
+  await enableDisableEventSourceMapping(config.SOURCE_LAMBDA_FUNCTION_NAME, false);
   const { versions, eventsReplayed, lastOffsetProcessed } = await replay();
+  await enableDisableEventSourceMapping(config.SOURCE_LAMBDA_FUNCTION_NAME, true);
   return { handler: "src/lib/main.replayBatchLambdaHandler", versions, eventsReplayed, lastOffsetProcessed };
 }
 
 export async function sourceLambdaHandler(sqsEvent) {
-  logInfo(`Source Lambda received event: ${JSON.stringify(sqsEvent, null, 2)}`);
+  logInfo(
+    `Source Lambda received event: ${JSON.stringify(sqsEvent, null, 2)}`
+  );
+
+  // If the latest bucket offset processed is null or behind the latest queue offset processed, error out, replay needed.
+  const { lastOffsetProcessed: replayQueueLastOffsetProcessed } = await readFromOffsetsTable(config.REPLAY_QUEUE_URL);
+  const { lastOffsetProcessed: bucketLastOffsetProcessed } = await readFromOffsetsTable(`${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`);
+  if (!bucketLastOffsetProcessed || bucketLastOffsetProcessed < replayQueueLastOffsetProcessed) {
+    throw new Error(`Bucket offset processed ${bucketLastOffsetProcessed} is behind replay queue offset processed ${replayQueueLastOffsetProcessed}. Replay needed.`);
+  }
 
-  // If event.Records is an array, use it.
-  // Otherwise, treat the event itself as one record.
+  // If event.Records is an array, use it. Otherwise, treat the event itself as one record.
   const sqsEventRecords = Array.isArray(sqsEvent.Records) ? sqsEvent.Records : [sqsEvent];
 
+  // Array to collect the identifiers of the failed records
+  const batchItemFailures = [];
+
   for (const sqsEventRecord of sqsEventRecords) {
-    const s3Event = parseMessageBody(sqsEventRecord.body);
-    await createProjections(s3Event);
-    logInfo(`Created source-projection.`);
+    try {
+      const s3Event = JSON.parse(sqsEventRecord.body);
+      const digest = await createProjections(s3Event);
+      await sendToSqs(digest, config.DIGEST_QUEUE_URL);
+      logInfo(`Created source-projection for with digest (and TODO dispatched to SQS): ${digest}`);
+    } catch (error) {
+      // Log the error and add the record's messageId to the partial batch response
+      logError(
+        `Error processing record ${sqsEventRecord.messageId}: ${error.message}`,
+        error
+      );
+      batchItemFailures.push({ itemIdentifier: sqsEventRecord.messageId });
+    }
   }
 
-  // TODO: When we have gathered a sample of events, compute the digests.
-
-  // TODO: Send the digest via SQS to decide if we should schedule an action.
-
-  return { handler: "src/lib/main.sourceLambdaHandler" };
+  // Return the list of failed messages so that AWS SQS can attempt to reprocess them.
+  return {
+    batchItemFailures,
+    handler: "src/lib/main.sourceLambdaHandler",
+  };
 }
 
 export async function replayLambdaHandler(sqsEvent) {
@@ -420,12 +519,36 @@ export async function replayLambdaHandler(sqsEvent) {
   // Otherwise, treat the event itself as one record.
   const sqsEventRecords = Array.isArray(sqsEvent.Records) ? sqsEvent.Records : [sqsEvent];
 
+  // Array to collect identifiers for records that failed processing
+  const batchItemFailures = [];
+
   for (const sqsEventRecord of sqsEventRecords) {
-    const s3Event = parseMessageBody(sqsEventRecord.body);
-    await createProjections(s3Event);
-    logInfo(`Created replay-projection.`);
+    try {
+      const s3Event = JSON.parse(sqsEventRecord.body);
+      const digest = await createProjections(s3Event);
+      // NOTE: Replay does not send the digest via SQS.
+      logInfo(`Created replay-projection for with digest: ${digest}`);
+    } catch (error) {
+      // Log the error and add the record's messageId to the partial batch response
+      logError(`Error processing record ${sqsEventRecord.messageId}: ${error.message}`, error);
+      batchItemFailures.push({ itemIdentifier: sqsEventRecord.messageId });
+    }
   }
-  return { handler: "src/lib/main.replayLambdaHandler" };
+
+  return {
+    handler: "src/lib/main.replayLambdaHandler",
+    batchItemFailures
+  };
+}
+
+// ---------------------------------------------------------------------------------------------------------------------
+// Health check server
+// ---------------------------------------------------------------------------------------------------------------------
+
+function healthCheckServer() {
+  const app = express();
+  app.get('/', (req, res) => res.send('S3 SQS Bridge OK'));
+  app.listen(8080, () => logInfo('Healthcheck available at :8080'));
 }
 
 // ---------------------------------------------------------------------------------------------------------------------

commit ed497f424567e4c13a2150abb25a6d304b0c67e0
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Sat Mar 22 01:10:12 2025 +0000

    Refactor event handling and add DynamoDB integration
    
    Refactored S3 and SQS event handling by modularizing event creation and processing for improved readability and reusability. Integrated DynamoDB tables for offset tracking and projections, and updated Lambda configurations to support this. Enhanced documentation and examples to align with the new functionality.

diff --git a/src/lib/main.js b/src/lib/main.js
index b6ae324..6bc16f6 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -188,7 +188,7 @@ export async function writeToProjectionsTable(item) {
 export async function writeToTable(item, params) {
   try {
     await dynamoClient.send(new PutItemCommand(params));
-    logInfo(`Successfully written offset to DynamoDB ${config.OFFSETS_TABLE_NAME}: ${JSON.stringify(item)}`);
+    logInfo(`Successfully written offset to DynamoDB ${config.OFFSETS_TABLE_NAME}`); // : ${JSON.stringify(item)}
   } catch (error) {
     logError("Error writing offset to DynamoDB", error);
     // Rethrow the error or handle it as needed
@@ -219,6 +219,66 @@ export async function retryOperationExponential(operation, retries = 3, delay =
   }
 }
 
+export function createS3EventFromVersion({key, versionId, lastModified}) {
+  return {
+    Records: [
+      {
+        eventVersion: "2.0",
+        eventSource: "aws:s3",
+        eventTime: lastModified,
+        eventName: "ObjectCreated:Put",
+        s3: {
+          s3SchemaVersion: "1.0",
+          bucket: {
+            name: config.BUCKET_NAME,
+            arn: "arn:aws:s3:::" + config.BUCKET_NAME
+          },
+          object: {
+            key,
+            versionId
+          }
+        }
+      }
+    ]
+  };
+}
+
+export function createSQSEventFromS3Event(s3Event) {
+  return {
+    Records: [
+      {
+        eventVersion: "2.0",
+        eventSource: "aws:sqs",
+        eventTime: s3Event.Records[0].eventTime,
+        eventName: "SendMessage",
+        body: JSON.stringify(s3Event)
+      }
+    ]
+  };
+}
+
+function streamToString(stream) {
+  return new Promise(function(resolve, reject) {
+    const chunks = [];
+    stream.on("data", function(chunk) {
+      chunks.push(chunk);
+    });
+    stream.on("error", function(err) {
+      reject(err);
+    });
+    stream.on("end", function() {
+      resolve(Buffer.concat(chunks).toString("utf-8"));
+    });
+  });
+}
+
+function getObjectContent(params, s3) {
+  return s3.send(new GetObjectCommand(params))
+    .then(function(object) {
+      return streamToString(object.Body);
+    });
+}
+
 // ---------------------------------------------------------------------------------------------------------------------
 // Utility functions
 // ---------------------------------------------------------------------------------------------------------------------
@@ -254,29 +314,9 @@ export async function replay() {
   });
   let eventsReplayed = 0;
   for (const version of versions) {
-    const event = {
-      Records: [
-        {
-          eventVersion: "2.0",
-          eventSource: "aws:s3",
-          eventTime: version.LastModified,
-          eventName: "ObjectCreated:Put",
-          s3: {
-            s3SchemaVersion: "1.0",
-            bucket: {
-              name: config.BUCKET_NAME,
-              arn: "arn:aws:s3:::" + config.BUCKET_NAME
-            },
-            object: {
-              key: version.Key,
-              versionId: version.VersionId
-            }
-          }
-        }
-      ]
-    }
+    const s3Event = createS3EventFromVersion({key: version.Key, versionId: version.VersionId, lastModified: version.LastModified});
 
-    await sendEventToSqs(event);
+    await sendEventToSqs(s3Event);
 
     await writeToOffsetsTable({
       id: config.REPLAY_QUEUE_URL,
@@ -294,32 +334,42 @@ export async function replay() {
 // Projection functions
 // ---------------------------------------------------------------------------------------------------------------------
 
-export async function createProjection(record) {
-  logInfo(`Creating projection from: ${record.body}...`);
-  const messageBody = parseMessageBody(record.body);
-  const records = messageBody.Records || [];
-  for (const record of records) {
-    if (record.eventName === 'ObjectCreated:Put') {
-      const id = messageBody.Records[0].s3.object.key;
-      const now = new Date().toISOString();
-      const params = {
-        Bucket: config.BUCKET_NAME,
-        Key: id
-      }
-      const version = await s3.send(new GetObjectCommand(params));
-      await writeToProjectionsTable({
-        id,
-        lastModified: now,
-        value: version.Body
-      });
-      await writeToOffsetsTable({
-        id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
-        lastModified: now,
-        lastOffsetProcessed: `${version.Key} ${version.LastModified} ${version.VersionId}`
-      });
+export async function createProjections(s3Event) {
+  logInfo(`Creating projections from: ${JSON.stringify(s3Event, null, 2)}...`);
+  const s3EventRecords = s3Event.Records || [];
+  for (const s3EventRecord of s3EventRecords) {
+    if (s3EventRecord.eventName === 'ObjectCreated:Put') {
+      await createProjection(s3EventRecord)
     } else {
-      logError(`Unsupported event name: ${record.eventName}`);
+      logError(`Unsupported event name: ${s3EventRecord.eventName}`);
+    }
+  }
+}
+
+export async function createProjection(s3PutEventRecord) {
+  if (s3PutEventRecord.eventName === 'ObjectCreated:Put') {
+    const id = s3PutEventRecord.s3.object.key;
+    const now = new Date().toISOString();
+    const params = {
+      Bucket: config.BUCKET_NAME,
+      Key: id,
+      VersionId: s3PutEventRecord.s3.object.versionId
     }
+    const object = await getObjectContent(params, s3);
+    //const object = await s3.send(new GetObjectCommand(params));
+    //logInfo(`Version body is: ${JSON.stringify(object.Body)}...`);
+    await writeToProjectionsTable({
+      id,
+      lastModified: now,
+      value: object.Body //JSON.stringify(object.Body)
+    });
+    await writeToOffsetsTable({
+      id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
+      lastModified: now,
+      lastOffsetProcessed: `${object.Key} ${object.LastModified} ${object.VersionId}`
+    });
+  } else {
+    logError(`Unsupported event name: ${s3PutEventRecord.eventName}`);
   }
 }
 
@@ -343,15 +393,16 @@ export async function replayBatchLambdaHandler(event) {
   return { handler: "src/lib/main.replayBatchLambdaHandler", versions, eventsReplayed, lastOffsetProcessed };
 }
 
-export async function sourceLambdaHandler(event) {
-  logInfo(`Source Lambda received event: ${JSON.stringify(event, null, 2)}`);
+export async function sourceLambdaHandler(sqsEvent) {
+  logInfo(`Source Lambda received event: ${JSON.stringify(sqsEvent, null, 2)}`);
 
   // If event.Records is an array, use it.
   // Otherwise, treat the event itself as one record.
-  const records = Array.isArray(event.Records) ? event.Records : [event];
+  const sqsEventRecords = Array.isArray(sqsEvent.Records) ? sqsEvent.Records : [sqsEvent];
 
-  for (const record of records) {
-    await createProjection(record);
+  for (const sqsEventRecord of sqsEventRecords) {
+    const s3Event = parseMessageBody(sqsEventRecord.body);
+    await createProjections(s3Event);
     logInfo(`Created source-projection.`);
   }
 
@@ -362,15 +413,16 @@ export async function sourceLambdaHandler(event) {
   return { handler: "src/lib/main.sourceLambdaHandler" };
 }
 
-export async function replayLambdaHandler(event) {
-  logInfo(`Replay Lambda received event: ${JSON.stringify(event, null, 2)}`);
+export async function replayLambdaHandler(sqsEvent) {
+  logInfo(`Replay Lambda received event: ${JSON.stringify(sqsEvent, null, 2)}`);
 
   // If event.Records is an array, use it.
   // Otherwise, treat the event itself as one record.
-  const records = Array.isArray(event.Records) ? event.Records : [event];
+  const sqsEventRecords = Array.isArray(sqsEvent.Records) ? sqsEvent.Records : [sqsEvent];
 
-  for (const record of records) {
-    await createProjection(record);
+  for (const sqsEventRecord of sqsEventRecords) {
+    const s3Event = parseMessageBody(sqsEventRecord.body);
+    await createProjections(s3Event);
     logInfo(`Created replay-projection.`);
   }
   return { handler: "src/lib/main.replayLambdaHandler" };
@@ -381,14 +433,21 @@ export async function replayLambdaHandler(event) {
 // ---------------------------------------------------------------------------------------------------------------------
 
 export async function main(args = process.argv.slice(2)) {
+  const exampleS3ObjectVersion = {
+    key: 'events/1.json',
+    versionId: 'AZW7UcKuQ.8ZZ5GnL9TaTMnK10xH1DON',
+    lastModified: new Date().toISOString()
+  }
   if (args.includes('--help')) {
     console.log(`
       Usage:
       --help                     Show this help message (default)
-      --source-projection        Run realtime Lambda handler
-      --replay-projection        Run replay Lambda handler
+      --source-projection        Run Lambda handler for events from source
+      --replay-projection        Run Lambda handler for events created by replay
       --replay                   Run full bucket replay
       --healthcheck              Run healthcheck server
+     Lambda handlers for --source-projection --replay-projection process a put event for the following object:
+     ${JSON.stringify(exampleS3ObjectVersion, null, 2)}
     `);
     return;
   }
@@ -396,9 +455,13 @@ export async function main(args = process.argv.slice(2)) {
   if (args.includes('--replay')) {
     await replay();
   } else if (args.includes('--source-projection')) {
-    await sourceLambdaHandler({ "source": "main" });
+    const s3Event = createS3EventFromVersion(exampleS3ObjectVersion);
+    const sqsEvent = createSQSEventFromS3Event(s3Event);
+    await sourceLambdaHandler(sqsEvent);
   } else if (args.includes('--replay-projection')) {
-    await replayLambdaHandler({ "source": "main" });
+    const s3Event = createS3EventFromVersion(exampleS3ObjectVersion);
+    const sqsEvent = createSQSEventFromS3Event(s3Event);
+    await replayLambdaHandler(sqsEvent);
   } else if (args.includes('--healthcheck')) {
     healthCheckServer();
   } else {

commit 4f57bbcd9b9bb5e77261d8f096586c1353e7ec7a
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Fri Mar 21 03:05:36 2025 +0000

    Refactor to handle optional fields and improve robustness.
    
    Updated logic to handle optional `lastOffsetProcessed` and `value` fields when writing to DynamoDB. Enhanced `createProjection` to support multiple records and log unsupported event types. Fixed attribute name `offset` to `lastModified` in S3SqsBridge stack configuration.

diff --git a/src/lib/main.js b/src/lib/main.js
index cd5ab9a..b6ae324 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -160,24 +160,26 @@ export async function sendEventToSqs(event) {
 }
 
 export async function writeToOffsetsTable(item) {
+  const lastOffsetProcessed = item.lastOffsetProcessed ? { S: item.lastOffsetProcessed } : null;
   const params = {
     TableName: config.OFFSETS_TABLE_NAME,
     Item: {
       id: { S: item.id },
       lastModified: { S: item.lastModified.toString() },
-      lastOffsetProcessed: { S: item.lastOffsetProcessed }
+      lastOffsetProcessed
     }
   };
   await writeToTable(item, params);
 }
 
 export async function writeToProjectionsTable(item) {
+  const value = item.value ? { S: item.value } : null;
   const params = {
     TableName: config.PROJECTIONS_TABLE_NAME,
     Item: {
       id: { S: item.id },
       lastModified: { S: item.lastModified.toString() },
-      value: { S: item.value }
+      value
     }
   };
   await writeToTable(item, params);
@@ -241,13 +243,16 @@ export async function replay() {
   logInfo(`Processing ${versions.length} versions...`);
   const latestVersion = versions[versions.length - 1];
   const now = new Date().toISOString();
+  let lastOffsetProcessed = null;
+  if (latestVersion) {
+    lastOffsetProcessed = `${latestVersion.Key} ${latestVersion.LastModified} ${latestVersion.VersionId}`;
+  }
   await writeToOffsetsTable({
     id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
     lastModified: now,
-    lastOffsetProcessed: `${latestVersion.Key} ${latestVersion.LastModified} ${latestVersion.VersionId}`
+    lastOffsetProcessed
   });
   let eventsReplayed = 0;
-  let lastOffsetProcessed = null;
   for (const version of versions) {
     const event = {
       Records: [
@@ -272,7 +277,6 @@ export async function replay() {
     }
 
     await sendEventToSqs(event);
-    lastOffsetProcessed = version.LastModified;
 
     await writeToOffsetsTable({
       id: config.REPLAY_QUEUE_URL,
@@ -293,23 +297,30 @@ export async function replay() {
 export async function createProjection(record) {
   logInfo(`Creating projection from: ${record.body}...`);
   const messageBody = parseMessageBody(record.body);
-  const id = messageBody.Records[0].s3.object.key;
-  const now = new Date().toISOString();
-  const params = {
-    Bucket: config.BUCKET_NAME,
-    Key: id
+  const records = messageBody.Records || [];
+  for (const record of records) {
+    if (record.eventName === 'ObjectCreated:Put') {
+      const id = messageBody.Records[0].s3.object.key;
+      const now = new Date().toISOString();
+      const params = {
+        Bucket: config.BUCKET_NAME,
+        Key: id
+      }
+      const version = await s3.send(new GetObjectCommand(params));
+      await writeToProjectionsTable({
+        id,
+        lastModified: now,
+        value: version.Body
+      });
+      await writeToOffsetsTable({
+        id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
+        lastModified: now,
+        lastOffsetProcessed: `${version.Key} ${version.LastModified} ${version.VersionId}`
+      });
+    } else {
+      logError(`Unsupported event name: ${record.eventName}`);
+    }
   }
-  const version = await s3.send(new GetObjectCommand(params));
-  await writeToProjectionsTable({
-    id,
-    lastModified: now,
-    value: version.Body
-  });
-  await writeToOffsetsTable({
-    id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
-    lastModified: now,
-    lastOffsetProcessed: `${version.Key} ${version.LastModified} ${version.VersionId}`
-  });
 }
 
 // ---------------------------------------------------------------------------------------------------------------------
@@ -343,6 +354,11 @@ export async function sourceLambdaHandler(event) {
     await createProjection(record);
     logInfo(`Created source-projection.`);
   }
+
+  // TODO: When we have gathered a sample of events, compute the digests.
+
+  // TODO: Send the digest via SQS to decide if we should schedule an action.
+
   return { handler: "src/lib/main.sourceLambdaHandler" };
 }
 

commit d90120c0fce010844c12144e859e14ff687ba934
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Fri Mar 21 01:34:46 2025 +0000

    Add DynamoDB integration for offsets and projections tracking
    
    Added DynamoDB client to write offset and projection data during replay and projection creation processes. Updated the schema of offsets and projections tables to align with new logic, replacing partition and sort key names for clarity. Improved AWS utility organization by restructuring functions.

diff --git a/src/lib/main.js b/src/lib/main.js
index abd5c79..cd5ab9a 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -7,6 +7,7 @@ import { z } from 'zod';
 import express from 'express';
 import { S3Client, ListObjectVersionsCommand, GetObjectCommand } from '@aws-sdk/client-s3';
 import { SQSClient, SendMessageCommand } from '@aws-sdk/client-sqs';
+import { DynamoDBClient, PutItemCommand } from '@aws-sdk/client-dynamodb';
 
 dotenv.config();
 
@@ -49,19 +50,10 @@ logConfig();
 
 const s3 = new S3Client({ endpoint: config.AWS_ENDPOINT, forcePathStyle: true });
 const sqs = new SQSClient({ endpoint: config.AWS_ENDPOINT });
-
-const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));
-
-function logInfo(message) {
-  console.log(JSON.stringify({ level: "info", timestamp: new Date().toISOString(), message }));
-}
-
-function logError(message, error) {
-  console.error(JSON.stringify({ level: "error", timestamp: new Date().toISOString(), message, error: error ? error.toString() : undefined }));
-}
+const dynamoClient = new DynamoDBClient({ endpoint: config.AWS_ENDPOINT });
 
 // ---------------------------------------------------------------------------------------------------------------------
-// Replay functions
+// AWS Utility functions
 // ---------------------------------------------------------------------------------------------------------------------
 
 export async function listAndSortAllObjectVersions() {
@@ -167,6 +159,41 @@ export async function sendEventToSqs(event) {
   }
 }
 
+export async function writeToOffsetsTable(item) {
+  const params = {
+    TableName: config.OFFSETS_TABLE_NAME,
+    Item: {
+      id: { S: item.id },
+      lastModified: { S: item.lastModified.toString() },
+      lastOffsetProcessed: { S: item.lastOffsetProcessed }
+    }
+  };
+  await writeToTable(item, params);
+}
+
+export async function writeToProjectionsTable(item) {
+  const params = {
+    TableName: config.PROJECTIONS_TABLE_NAME,
+    Item: {
+      id: { S: item.id },
+      lastModified: { S: item.lastModified.toString() },
+      value: { S: item.value }
+    }
+  };
+  await writeToTable(item, params);
+}
+
+export async function writeToTable(item, params) {
+  try {
+    await dynamoClient.send(new PutItemCommand(params));
+    logInfo(`Successfully written offset to DynamoDB ${config.OFFSETS_TABLE_NAME}: ${JSON.stringify(item)}`);
+  } catch (error) {
+    logError("Error writing offset to DynamoDB", error);
+    // Rethrow the error or handle it as needed
+    throw error;
+  }
+}
+
 export function parseMessageBody(text) {
   try {
     return JSON.parse(text);
@@ -190,10 +217,35 @@ export async function retryOperationExponential(operation, retries = 3, delay =
   }
 }
 
+// ---------------------------------------------------------------------------------------------------------------------
+// Utility functions
+// ---------------------------------------------------------------------------------------------------------------------
+
+const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));
+
+function logInfo(message) {
+  console.log(JSON.stringify({ level: "info", timestamp: new Date().toISOString(), message }));
+}
+
+function logError(message, error) {
+  console.error(JSON.stringify({ level: "error", timestamp: new Date().toISOString(), message, error: error ? error.toString() : undefined }));
+}
+
+// ---------------------------------------------------------------------------------------------------------------------
+// Replay functions
+// ---------------------------------------------------------------------------------------------------------------------
+
 export async function replay() {
   logInfo(`Starting replay job for bucket ${config.BUCKET_NAME} prefix ${config.OBJECT_PREFIX}`);
   const versions = await listAllObjectVersionsOldestFirst();
   logInfo(`Processing ${versions.length} versions...`);
+  const latestVersion = versions[versions.length - 1];
+  const now = new Date().toISOString();
+  await writeToOffsetsTable({
+    id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
+    lastModified: now,
+    lastOffsetProcessed: `${latestVersion.Key} ${latestVersion.LastModified} ${latestVersion.VersionId}`
+  });
   let eventsReplayed = 0;
   let lastOffsetProcessed = null;
   for (const version of versions) {
@@ -221,6 +273,13 @@ export async function replay() {
 
     await sendEventToSqs(event);
     lastOffsetProcessed = version.LastModified;
+
+    await writeToOffsetsTable({
+      id: config.REPLAY_QUEUE_URL,
+      lastModified: now,
+      lastOffsetProcessed: `${version.Key} ${version.LastModified} ${version.VersionId}`
+    });
+
     eventsReplayed++;
   }
   logInfo('replay job complete.');
@@ -233,7 +292,24 @@ export async function replay() {
 
 export async function createProjection(record) {
   logInfo(`Creating projection from: ${record.body}...`);
-  // TODO Create projections in the database
+  const messageBody = parseMessageBody(record.body);
+  const id = messageBody.Records[0].s3.object.key;
+  const now = new Date().toISOString();
+  const params = {
+    Bucket: config.BUCKET_NAME,
+    Key: id
+  }
+  const version = await s3.send(new GetObjectCommand(params));
+  await writeToProjectionsTable({
+    id,
+    lastModified: now,
+    value: version.Body
+  });
+  await writeToOffsetsTable({
+    id: `${config.BUCKET_NAME}/${config.OBJECT_PREFIX}`,
+    lastModified: now,
+    lastOffsetProcessed: `${version.Key} ${version.LastModified} ${version.VersionId}`
+  });
 }
 
 // ---------------------------------------------------------------------------------------------------------------------

commit 3aaaa1cbe914e9e1f99f696511754c22eba5b6db
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Fri Mar 21 00:28:09 2025 +0000

    Add listAllObjectVersionsOldestFirst and update AWS CDK version
    
    Introduced a new function `listAllObjectVersionsOldestFirst` to retrieve S3 object versions in upload order. Updated `replay` to utilize the new function for better version sorting. Additionally, bumped AWS CDK devDependency to version 2.1005.0.

diff --git a/src/lib/main.js b/src/lib/main.js
index 47f18f6..abd5c79 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -87,6 +87,66 @@ export async function listAndSortAllObjectVersions() {
   return versions;
 }
 
+export async function listAllObjectVersionsOldestFirst() {
+  let versions = [];
+  let params = {
+    Bucket: config.BUCKET_NAME,
+    Prefix: config.OBJECT_PREFIX
+  };
+  let response;
+  do {
+    response = await s3.send(new ListObjectVersionsCommand(params));
+    if (response.Versions) {
+      versions.push(...response.Versions);
+      params.KeyMarker = response.NextKeyMarker;
+      params.VersionIdMarker = response.NextVersionIdMarker;
+    } else {
+      logInfo(`No versions found in the response for ${config.BUCKET_NAME}: ${JSON.stringify(response)}`);
+      break;
+    }
+  } while (response.IsTruncated);
+
+  // Group versions by object key.
+  const grouped = versions.reduce((acc, version) => {
+    const key = version.Key;
+    if (!acc[key]) {
+      acc[key] = [];
+    }
+    acc[key].push(version);
+    return acc;
+  }, {});
+
+  // For each key, reverse the array so that versions are in upload order (oldest first)
+  Object.keys(grouped).forEach(key => {
+    grouped[key] = grouped[key].reverse();
+  });
+
+  // Now merge the sorted arrays (each group) into a single list ordered by LastModified.
+  // This is a k-way merge.
+  const lists = Object.values(grouped); // each is an array sorted oldest-first
+  const merged = [];
+
+  while (lists.some(list => list.length > 0)) {
+    // Find the list with the smallest (oldest) head element.
+    let minIndex = -1;
+    let minVersion = null;
+    for (let i = 0; i < lists.length; i++) {
+      if (lists[i].length > 0) {
+        const candidate = lists[i][0];
+        if (!minVersion || new Date(candidate.LastModified) < new Date(minVersion.LastModified)) {
+          minVersion = candidate;
+          minIndex = i;
+        }
+      }
+    }
+    // Remove the smallest head element and push it to the merged list.
+    if (minIndex >= 0) {
+      merged.push(lists[minIndex].shift());
+    }
+  }
+  return merged;
+}
+
 export function buildSQSMessageParams(event) {
   return {
     QueueUrl: config.REPLAY_QUEUE_URL,
@@ -132,7 +192,7 @@ export async function retryOperationExponential(operation, retries = 3, delay =
 
 export async function replay() {
   logInfo(`Starting replay job for bucket ${config.BUCKET_NAME} prefix ${config.OBJECT_PREFIX}`);
-  const versions = await listAndSortAllObjectVersions();
+  const versions = await listAllObjectVersionsOldestFirst();
   logInfo(`Processing ${versions.length} versions...`);
   let eventsReplayed = 0;
   let lastOffsetProcessed = null;

commit 9bf6461eaca69dbb5418841c108bd79af4c054fa
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Thu Mar 20 00:24:37 2025 +0000

    Add projections and offsets table support with enhanced Lambdas.
    
    Introduced support for new Projections and Offsets DynamoDB tables to handle replay and event sourcing workflows. Enhanced Lambda functions (ReplayLambda, SourceLambda, ReplayBatchLambda) with updated responsibilities for creating projections and replay tasks. Updated the CDK stack to reflect these changes, including resource, role, and permission enhancements.

diff --git a/src/lib/main.js b/src/lib/main.js
index bc313fb..47f18f6 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -14,13 +14,17 @@ if (process.env.VITEST || process.env.NODE_ENV === "development") {
   process.env.BUCKET_NAME = process.env.BUCKET_NAME || " s3-sqs-bridge-bucket-test";
   process.env.OBJECT_PREFIX = process.env.OBJECT_PREFIX || "events/";
   process.env.REPLAY_QUEUE_URL = process.env.REPLAY_QUEUE_URL || "http://test/000000000000/s3-sqs-bridge-replay-queue-test";
+  process.env.OFFSETS_TABLE_NAME = process.env.OFFSETS_TABLE_NAME || "s3-sqs-bridge-offsets-table-test";
+  process.env.PROJECTIONS_TABLE_NAME = process.env.PROJECTIONS_TABLE_NAME || "s3-sqs-bridge-projections-table-test";
   process.env.AWS_ENDPOINT = process.env.AWS_ENDPOINT || "http://test";
 }
 
 const configSchema = z.object({
-  BUCKET_NAME: z.string().nonempty(),
+  BUCKET_NAME: z.string().optional(),
   OBJECT_PREFIX: z.string().optional(),
-  REPLAY_QUEUE_URL: z.string().nonempty(),
+  REPLAY_QUEUE_URL: z.string().optional(),
+  OFFSETS_TABLE_NAME: z.string().optional(),
+  PROJECTIONS_TABLE_NAME: z.string().optional(),
   AWS_ENDPOINT: z.string().optional()
 });
 
@@ -35,13 +39,14 @@ function logConfig() {
       BUCKET_NAME: config.BUCKET_NAME,
       OBJECT_PREFIX: config.OBJECT_PREFIX,
       REPLAY_QUEUE_URL: config.REPLAY_QUEUE_URL,
+      OFFSETS_TABLE_NAME: config.OFFSETS_TABLE_NAME,
+      PROJECTIONS_TABLE_NAME: config.PROJECTIONS_TABLE_NAME,
       AWS_ENDPOINT: config.AWS_ENDPOINT
     }
   }));
 }
 logConfig();
 
-// Use the separate endpoints if provided; otherwise use AWS_ENDPOINT.
 const s3 = new S3Client({ endpoint: config.AWS_ENDPOINT, forcePathStyle: true });
 const sqs = new SQSClient({ endpoint: config.AWS_ENDPOINT });
 
@@ -56,7 +61,7 @@ function logError(message, error) {
 }
 
 // ---------------------------------------------------------------------------------------------------------------------
-// Replay Task
+// Replay functions
 // ---------------------------------------------------------------------------------------------------------------------
 
 export async function listAndSortAllObjectVersions() {
@@ -129,16 +134,46 @@ export async function replay() {
   logInfo(`Starting replay job for bucket ${config.BUCKET_NAME} prefix ${config.OBJECT_PREFIX}`);
   const versions = await listAndSortAllObjectVersions();
   logInfo(`Processing ${versions.length} versions...`);
+  let eventsReplayed = 0;
+  let lastOffsetProcessed = null;
   for (const version of versions) {
     const event = {
-      bucket: config.BUCKET_NAME,
-      key: version.Key,
-      versionId: version.VersionId,
-      eventTime: version.LastModified
-    };
+      Records: [
+        {
+          eventVersion: "2.0",
+          eventSource: "aws:s3",
+          eventTime: version.LastModified,
+          eventName: "ObjectCreated:Put",
+          s3: {
+            s3SchemaVersion: "1.0",
+            bucket: {
+              name: config.BUCKET_NAME,
+              arn: "arn:aws:s3:::" + config.BUCKET_NAME
+            },
+            object: {
+              key: version.Key,
+              versionId: version.VersionId
+            }
+          }
+        }
+      ]
+    }
+
     await sendEventToSqs(event);
+    lastOffsetProcessed = version.LastModified;
+    eventsReplayed++;
   }
   logInfo('replay job complete.');
+  return { versions: versions.length, eventsReplayed, lastOffsetProcessed };
+}
+
+// ---------------------------------------------------------------------------------------------------------------------
+// Projection functions
+// ---------------------------------------------------------------------------------------------------------------------
+
+export async function createProjection(record) {
+  logInfo(`Creating projection from: ${record.body}...`);
+  // TODO Create projections in the database
 }
 
 // ---------------------------------------------------------------------------------------------------------------------
@@ -157,37 +192,36 @@ function healthCheckServer() {
 
 export async function replayBatchLambdaHandler(event) {
   logInfo(`Replay Batch Lambda received event: ${JSON.stringify(event, null, 2)}`);
+  const { versions, eventsReplayed, lastOffsetProcessed } = await replay();
+  return { handler: "src/lib/main.replayBatchLambdaHandler", versions, eventsReplayed, lastOffsetProcessed };
+}
+
+export async function sourceLambdaHandler(event) {
+  logInfo(`Source Lambda received event: ${JSON.stringify(event, null, 2)}`);
 
   // If event.Records is an array, use it.
   // Otherwise, treat the event itself as one record.
   const records = Array.isArray(event.Records) ? event.Records : [event];
 
   for (const record of records) {
-    // If the record has a "body", then it is likely coming from an SQS event.
-    // Otherwise, log the entire record.
-    const message = record.body ? record.body : JSON.stringify(record);
-    logInfo(`Create replay batch from: ${message}.`);
-  }
-
-  await replay();
-  return { status: "logged" };
-}
-
-
-export async function sourceLambdaHandler(event) {
-  logInfo(`Source Lambda received event: ${JSON.stringify(event, null, 2)}`);
-  for (const record of event.Records) {
-    logInfo(`Create source-projection from: ${record.body}.`);
+    await createProjection(record);
+    logInfo(`Created source-projection.`);
   }
-  return { status: "logged" };
+  return { handler: "src/lib/main.sourceLambdaHandler" };
 }
 
 export async function replayLambdaHandler(event) {
   logInfo(`Replay Lambda received event: ${JSON.stringify(event, null, 2)}`);
-  for (const record of event.Records) {
-    logInfo(`Create replay-projection from: ${record.body}.`);
+
+  // If event.Records is an array, use it.
+  // Otherwise, treat the event itself as one record.
+  const records = Array.isArray(event.Records) ? event.Records : [event];
+
+  for (const record of records) {
+    await createProjection(record);
+    logInfo(`Created replay-projection.`);
   }
-  return { status: "logged" };
+  return { handler: "src/lib/main.replayLambdaHandler" };
 }
 
 // ---------------------------------------------------------------------------------------------------------------------

commit ce92f29131c726725a23da70e13665f4244c76cf
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Wed Mar 19 20:55:46 2025 +0000

    Refactor Lambda setup and remove redundant entrypoint script.
    
    Replaced `entrypoint.sh` with Lambda environment-based handler configuration. Simplified Dockerfile, switching to AWS Lambda base image and adding dynamic handler support. Updated S3SqsBridgeStack to use `DockerImageFunction` and added improved S3 access policies.

diff --git a/src/lib/main.js b/src/lib/main.js
index 40f8c3b..bc313fb 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -68,9 +68,14 @@ export async function listAndSortAllObjectVersions() {
   let response;
   do {
     response = await s3.send(new ListObjectVersionsCommand(params));
-    versions.push(...response.Versions);
-    params.KeyMarker = response.NextKeyMarker;
-    params.VersionIdMarker = response.NextVersionIdMarker;
+    if(response.Versions) {
+      versions.push(...response.Versions);
+      params.KeyMarker = response.NextKeyMarker;
+      params.VersionIdMarker = response.NextVersionIdMarker;
+    }else {
+      logInfo(`No versions found in the response for ${config.BUCKET_NAME}: ${JSON.stringify(response)}`);
+      break;
+    }
   } while (response.IsTruncated);
 
   versions.sort((a, b) => new Date(a.LastModified) - new Date(b.LastModified));
@@ -152,13 +157,23 @@ function healthCheckServer() {
 
 export async function replayBatchLambdaHandler(event) {
   logInfo(`Replay Batch Lambda received event: ${JSON.stringify(event, null, 2)}`);
-  for (const record of event.Records) {
-    logInfo(`Create replay batch from: ${record.body}.`);
+
+  // If event.Records is an array, use it.
+  // Otherwise, treat the event itself as one record.
+  const records = Array.isArray(event.Records) ? event.Records : [event];
+
+  for (const record of records) {
+    // If the record has a "body", then it is likely coming from an SQS event.
+    // Otherwise, log the entire record.
+    const message = record.body ? record.body : JSON.stringify(record);
+    logInfo(`Create replay batch from: ${message}.`);
   }
+
   await replay();
   return { status: "logged" };
 }
 
+
 export async function sourceLambdaHandler(event) {
   logInfo(`Source Lambda received event: ${JSON.stringify(event, null, 2)}`);
   for (const record of event.Records) {

commit 9e062874b32eddae104591fb20a3437dfa596bf4
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Wed Mar 19 02:00:09 2025 +0000

    Add replayBatchLambdaHandler and infrastructure support
    
    Introduced a new `replayBatchLambdaHandler` function for batch processing in the Lambda. Updated the CDK stack to provision the necessary Lambda, custom resource, and outputs for this handler, ensuring seamless deployment and integration. Removed unused ECS and App Runner configurations for simplification.

diff --git a/src/lib/main.js b/src/lib/main.js
index 6f29eca..40f8c3b 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -150,6 +150,15 @@ function healthCheckServer() {
 // SQS Lambda Handlers
 // ---------------------------------------------------------------------------------------------------------------------
 
+export async function replayBatchLambdaHandler(event) {
+  logInfo(`Replay Batch Lambda received event: ${JSON.stringify(event, null, 2)}`);
+  for (const record of event.Records) {
+    logInfo(`Create replay batch from: ${record.body}.`);
+  }
+  await replay();
+  return { status: "logged" };
+}
+
 export async function sourceLambdaHandler(event) {
   logInfo(`Source Lambda received event: ${JSON.stringify(event, null, 2)}`);
   for (const record of event.Records) {

commit 457976239c4b47265cc307e8973b1d6749ec8be0
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Wed Mar 19 00:39:14 2025 +0000

    Update package dependencies to latest versions
    
    Upgraded various @rollup and @vitest packages to their latest versions, including @rollup (4.36.0) and @vitest (3.0.9). Ensures compatibility, bug fixes, and improved performance across the updated modules.

diff --git a/src/lib/main.js b/src/lib/main.js
index 7ed998d..6f29eca 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -10,47 +10,40 @@ import { SQSClient, SendMessageCommand } from '@aws-sdk/client-sqs';
 
 dotenv.config();
 
-// --------------------
-// For test or development environment, supply default env values to avoid configuration errors.
-// In production, ensure all required environment variables are set.
-// --------------------
 if (process.env.VITEST || process.env.NODE_ENV === "development") {
-  process.env.BUCKET_NAME = process.env.BUCKET_NAME || "test";
-  process.env.OBJECT_PREFIX = process.env.OBJECT_PREFIX || "test/";
-  process.env.SOURCE_QUEUE_URL = process.env.SOURCE_QUEUE_URL || "https://sqs.eu-west-2.amazonaws.com/000000000000/source-queue-test";
-  process.env.REPLAY_QUEUE_URL = process.env.REPLAY_QUEUE_URL || "https://sqs.eu-west-2.amazonaws.com/000000000000/replay-queue-test";
-  process.env.S3_ENDPOINT = process.env.S3_ENDPOINT || "https://s3.eu-west-2.amazonaws.com";
-  process.env.SQS_ENDPOINT = process.env.SQS_ENDPOINT || "https://sqs.eu-west-2.amazonaws.com";
-  process.env.AWS_ENDPOINT = process.env.AWS_ENDPOINT || "https://s3.eu-west-2.amazonaws.com";
+  process.env.BUCKET_NAME = process.env.BUCKET_NAME || " s3-sqs-bridge-bucket-test";
+  process.env.OBJECT_PREFIX = process.env.OBJECT_PREFIX || "events/";
+  process.env.REPLAY_QUEUE_URL = process.env.REPLAY_QUEUE_URL || "http://test/000000000000/s3-sqs-bridge-replay-queue-test";
+  process.env.AWS_ENDPOINT = process.env.AWS_ENDPOINT || "http://test";
 }
 
 const configSchema = z.object({
   BUCKET_NAME: z.string().nonempty(),
   OBJECT_PREFIX: z.string().optional(),
-  SOURCE_QUEUE_URL: z.string().nonempty(),
   REPLAY_QUEUE_URL: z.string().nonempty(),
-  // Allow separate endpoints for S3 and SQS for local testing. If not provided, fall back to AWS_ENDPOINT.
-  S3_ENDPOINT: z.string().optional(),
-  SQS_ENDPOINT: z.string().optional(),
   AWS_ENDPOINT: z.string().optional()
 });
 
 const config = configSchema.parse(process.env);
 
-// Log non-sensitive configuration details.
 function logConfig() {
   console.log(JSON.stringify({
     level: "info",
     timestamp: new Date().toISOString(),
     message: "Configuration loaded",
-    config: { BUCKET_NAME: config.BUCKET_NAME, OBJECT_PREFIX: config.OBJECT_PREFIX, SOURCE_QUEUE_URL: config.SOURCE_QUEUE_URL, REPLAY_QUEUE_URL: config.REPLAY_QUEUE_URL }
+    config: {
+      BUCKET_NAME: config.BUCKET_NAME,
+      OBJECT_PREFIX: config.OBJECT_PREFIX,
+      REPLAY_QUEUE_URL: config.REPLAY_QUEUE_URL,
+      AWS_ENDPOINT: config.AWS_ENDPOINT
+    }
   }));
 }
 logConfig();
 
 // Use the separate endpoints if provided; otherwise use AWS_ENDPOINT.
-const s3 = new S3Client({ endpoint: config.S3_ENDPOINT || config.AWS_ENDPOINT });
-const sqs = new SQSClient({ endpoint: config.SQS_ENDPOINT || config.AWS_ENDPOINT });
+const s3 = new S3Client({ endpoint: config.AWS_ENDPOINT, forcePathStyle: true });
+const sqs = new SQSClient({ endpoint: config.AWS_ENDPOINT });
 
 const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));
 
@@ -97,9 +90,9 @@ export async function sendEventToSqs(event) {
     const result = await retryOperationExponential(async () =>
         await sqs.send(new SendMessageCommand(params))
     );
-    logInfo(`Sent message to SQS, MessageId: ${result.MessageId}`);
+    logInfo(`Sent message to SQS queue ${config.REPLAY_QUEUE_URL}, MessageId: ${result.MessageId}`);
   } catch (err) {
-    logError("Failed to send message to SQS", err);
+    logError(`Failed to send message to SQS queue ${config.REPLAY_QUEUE_URL}`, err);
     throw err;
   }
 }
@@ -181,6 +174,7 @@ export async function main(args = process.argv.slice(2)) {
   if (args.includes('--help')) {
     console.log(`
       Usage:
+      --help                     Show this help message (default)
       --source-projection        Run realtime Lambda handler
       --replay-projection        Run replay Lambda handler
       --replay                   Run full bucket replay
@@ -198,7 +192,7 @@ export async function main(args = process.argv.slice(2)) {
   } else if (args.includes('--healthcheck')) {
     healthCheckServer();
   } else {
-    console.log('Invalid or missing argument. Run with --help for usage.');
+    console.log('No command argument supplied.');
   }
 }
 

commit 04217e53704decaf5801ea9e8f2465cde127c5b2
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Tue Mar 18 01:56:49 2025 +0000

    Refactor S3 replay and event-processing architecture
    
    Enhanced replay and real-time event processing by introducing distinct SQS queues for source and replay workflows. Updated environment configuration, added Lambda functions for SQS processing, introduced DynamoDB for offset tracking, and refined the ECS and Docker setup. README and tests were updated to reflect these changes.

diff --git a/src/lib/main.js b/src/lib/main.js
index c5e46c7..7ed998d 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -10,9 +10,25 @@ import { SQSClient, SendMessageCommand } from '@aws-sdk/client-sqs';
 
 dotenv.config();
 
+// --------------------
+// For test or development environment, supply default env values to avoid configuration errors.
+// In production, ensure all required environment variables are set.
+// --------------------
+if (process.env.VITEST || process.env.NODE_ENV === "development") {
+  process.env.BUCKET_NAME = process.env.BUCKET_NAME || "test";
+  process.env.OBJECT_PREFIX = process.env.OBJECT_PREFIX || "test/";
+  process.env.SOURCE_QUEUE_URL = process.env.SOURCE_QUEUE_URL || "https://sqs.eu-west-2.amazonaws.com/000000000000/source-queue-test";
+  process.env.REPLAY_QUEUE_URL = process.env.REPLAY_QUEUE_URL || "https://sqs.eu-west-2.amazonaws.com/000000000000/replay-queue-test";
+  process.env.S3_ENDPOINT = process.env.S3_ENDPOINT || "https://s3.eu-west-2.amazonaws.com";
+  process.env.SQS_ENDPOINT = process.env.SQS_ENDPOINT || "https://sqs.eu-west-2.amazonaws.com";
+  process.env.AWS_ENDPOINT = process.env.AWS_ENDPOINT || "https://s3.eu-west-2.amazonaws.com";
+}
+
 const configSchema = z.object({
   BUCKET_NAME: z.string().nonempty(),
-  QUEUE_URL: z.string().nonempty(),
+  OBJECT_PREFIX: z.string().optional(),
+  SOURCE_QUEUE_URL: z.string().nonempty(),
+  REPLAY_QUEUE_URL: z.string().nonempty(),
   // Allow separate endpoints for S3 and SQS for local testing. If not provided, fall back to AWS_ENDPOINT.
   S3_ENDPOINT: z.string().optional(),
   SQS_ENDPOINT: z.string().optional(),
@@ -27,7 +43,7 @@ function logConfig() {
     level: "info",
     timestamp: new Date().toISOString(),
     message: "Configuration loaded",
-    config: { BUCKET_NAME: config.BUCKET_NAME, QUEUE_URL: config.QUEUE_URL }
+    config: { BUCKET_NAME: config.BUCKET_NAME, OBJECT_PREFIX: config.OBJECT_PREFIX, SOURCE_QUEUE_URL: config.SOURCE_QUEUE_URL, REPLAY_QUEUE_URL: config.REPLAY_QUEUE_URL }
   }));
 }
 logConfig();
@@ -46,9 +62,16 @@ function logError(message, error) {
   console.error(JSON.stringify({ level: "error", timestamp: new Date().toISOString(), message, error: error ? error.toString() : undefined }));
 }
 
-async function listAndSortAllObjectVersions() {
+// ---------------------------------------------------------------------------------------------------------------------
+// Replay Task
+// ---------------------------------------------------------------------------------------------------------------------
+
+export async function listAndSortAllObjectVersions() {
   let versions = [];
-  let params = { Bucket: config.BUCKET_NAME };
+  let params = {
+    Bucket: config.BUCKET_NAME,
+    Prefix: config.OBJECT_PREFIX
+  };
   let response;
   do {
     response = await s3.send(new ListObjectVersionsCommand(params));
@@ -61,14 +84,14 @@ async function listAndSortAllObjectVersions() {
   return versions;
 }
 
-function buildSQSMessageParams(event) {
+export function buildSQSMessageParams(event) {
   return {
-    QueueUrl: config.QUEUE_URL,
+    QueueUrl: config.REPLAY_QUEUE_URL,
     MessageBody: JSON.stringify(event)
   };
 }
 
-async function sendEventToSqs(event) {
+export async function sendEventToSqs(event) {
   const params = buildSQSMessageParams(event);
   try {
     const result = await retryOperationExponential(async () =>
@@ -81,7 +104,7 @@ async function sendEventToSqs(event) {
   }
 }
 
-function parseMessageBody(text) {
+export function parseMessageBody(text) {
   try {
     return JSON.parse(text);
   } catch (e) {
@@ -89,11 +112,7 @@ function parseMessageBody(text) {
   }
 }
 
-function validateConfig(cfg) {
-  return cfg.BUCKET_NAME && cfg.QUEUE_URL;
-}
-
-async function retryOperationExponential(operation, retries = 3, delay = 100) {
+export async function retryOperationExponential(operation, retries = 3, delay = 100) {
   let attempt = 0;
   while (attempt < retries) {
     try {
@@ -108,23 +127,8 @@ async function retryOperationExponential(operation, retries = 3, delay = 100) {
   }
 }
 
-export async function realtimeLambdaHandler(event) {
-  logInfo(`Received realtime event: ${JSON.stringify(event)}`);
-  for (const record of event.Records) {
-    const { s3 } = record;
-    const eventDetail = {
-      bucket: s3.bucket.name,
-      key: s3.object.key,
-      eventTime: record.eventTime,
-      versionId: s3.object.versionId,
-      sequencer: s3.object.sequencer
-    };
-    await sendEventToSqs(eventDetail);
-  }
-}
-
-export async function reseed() {
-  logInfo(`Starting reseed job for bucket ${config.BUCKET_NAME}`);
+export async function replay() {
+  logInfo(`Starting replay job for bucket ${config.BUCKET_NAME} prefix ${config.OBJECT_PREFIX}`);
   const versions = await listAndSortAllObjectVersions();
   logInfo(`Processing ${versions.length} versions...`);
   for (const version of versions) {
@@ -136,30 +140,61 @@ export async function reseed() {
     };
     await sendEventToSqs(event);
   }
-  logInfo('Reseed job complete.');
+  logInfo('replay job complete.');
 }
 
+// ---------------------------------------------------------------------------------------------------------------------
+// Health check server
+// ---------------------------------------------------------------------------------------------------------------------
+
 function healthCheckServer() {
   const app = express();
   app.get('/', (req, res) => res.send('S3 SQS Bridge OK'));
   app.listen(8080, () => logInfo('Healthcheck available at :8080'));
 }
 
-async function main(args = process.argv.slice(2)) {
+// ---------------------------------------------------------------------------------------------------------------------
+// SQS Lambda Handlers
+// ---------------------------------------------------------------------------------------------------------------------
+
+export async function sourceLambdaHandler(event) {
+  logInfo(`Source Lambda received event: ${JSON.stringify(event, null, 2)}`);
+  for (const record of event.Records) {
+    logInfo(`Create source-projection from: ${record.body}.`);
+  }
+  return { status: "logged" };
+}
+
+export async function replayLambdaHandler(event) {
+  logInfo(`Replay Lambda received event: ${JSON.stringify(event, null, 2)}`);
+  for (const record of event.Records) {
+    logInfo(`Create replay-projection from: ${record.body}.`);
+  }
+  return { status: "logged" };
+}
+
+// ---------------------------------------------------------------------------------------------------------------------
+// Main CLI
+// ---------------------------------------------------------------------------------------------------------------------
+
+export async function main(args = process.argv.slice(2)) {
   if (args.includes('--help')) {
     console.log(`
       Usage:
-      --realtime-lambda          Run realtime Lambda handler
-      --reseed                   Run full bucket reseed
+      --source-projection        Run realtime Lambda handler
+      --replay-projection        Run replay Lambda handler
+      --replay                   Run full bucket replay
       --healthcheck              Run healthcheck server
     `);
     return;
   }
 
-  if (args.includes('--reseed')) {
-    await reseed();
-  } else if (args.includes('--realtime-lambda')) {
-    console.log('Realtime lambda handler requires AWS Lambda invocation context.');
+  if (args.includes('--replay')) {
+    await replay();
+  } else if (args.includes('--source-projection')) {
+    await sourceLambdaHandler({ "source": "main" });
+  } else if (args.includes('--replay-projection')) {
+    await replayLambdaHandler({ "source": "main" });
   } else if (args.includes('--healthcheck')) {
     healthCheckServer();
   } else {
@@ -174,4 +209,3 @@ if (import.meta.url.endsWith(process.argv[1])) {
   });
 }
 
-export { main, parseMessageBody, buildSQSMessageParams, validateConfig, sendEventToSqs, retryOperationExponential, listAndSortAllObjectVersions };

commit f9786bf60bd4393328f42568202dde4d654b77dd
Author: Antony at Polycode <antony@polycode.co.uk>
Date:   Mon Mar 17 02:21:37 2025 +0000

    Rename project and update README for new functionality
    
    Renamed project from "tansu-sqs-bridge" to "S3 SQS Bridge" to align with its updated purpose of handling versioned event replays in S3. Updated README to provide detailed documentation, including architecture overview, features, setup instructions, and cost analysis.

diff --git a/src/lib/main.js b/src/lib/main.js
index 2278396..c5e46c7 100755
--- a/src/lib/main.js
+++ b/src/lib/main.js
@@ -1,383 +1,177 @@
 #!/usr/bin/env node
 // src/lib/main.js
-// Tansu SQS Bridge - Aligned with our mission statement (v0.1.5)
+// S3 SQS Bridge (v0.2.0) - S3 event sourcing, SQS bridging, Lambda projections.
 
-// Ensure NODE_ENV is set to development by default for local/test runs
-process.env.NODE_ENV = process.env.NODE_ENV || "development";
+import dotenv from 'dotenv';
+import { z } from 'zod';
+import express from 'express';
+import { S3Client, ListObjectVersionsCommand, GetObjectCommand } from '@aws-sdk/client-s3';
+import { SQSClient, SendMessageCommand } from '@aws-sdk/client-sqs';
 
-import { fileURLToPath } from "url";
-import process from "process";
-import dotenv from "dotenv";
-import { z } from "zod";
-import { Kafka } from "kafkajs";
-import { SQSClient, SendMessageCommand } from "@aws-sdk/client-sqs";
-import pkg from "pg";
-import express from "express";
+dotenv.config();
 
-const { Client: PGClient } = pkg;
-
-// --------------------
-// For test or development environment, supply default env values to avoid configuration errors.
-// In production, ensure all required environment variables are set.
-// --------------------
-if (process.env.VITEST || process.env.NODE_ENV === "development") {
-  process.env.SQS_QUEUE_URL = process.env.SQS_QUEUE_URL || "https://sqs.eu-west-2.amazonaws.com/000000000000/test";
-  process.env.BROKER_URL = process.env.BROKER_URL || "localhost:9092";
-  process.env.TOPIC_NAME = process.env.TOPIC_NAME || "test";
-  process.env.USE_EXISTING_TOPIC = process.env.USE_EXISTING_TOPIC || "false";
-  process.env.CONSUMER_GROUP = process.env.CONSUMER_GROUP || "tansu-sqs-bridge-group";
-  // process.env.PGHOST = process.env.PGHOST || "localhost";
-  // process.env.PGUSER = process.env.PGUSER || "test";
-  // process.env.PGPASSWORD = process.env.PGPASSWORD || "test";
-  // process.env.PGDATABASE = process.env.PGDATABASE || "test";
-  // process.env.PGPORT = process.env.PGPORT || "5432";
-}
-
-// --------------------
-// Environment configuration schema using zod
-// --------------------
 const configSchema = z.object({
-  // Kafka settings
-  BROKER_URL: z.string().nonempty({ message: "BROKER_URL is required" }),
-  TOPIC_NAME: z.string().nonempty({ message: "TOPIC_NAME is required" }),
-  USE_EXISTING_TOPIC: z.string().nonempty({ message: "USE_EXISTING_TOPIC is required" }),
-  CONSUMER_GROUP: z.string().nonempty({ message: "CONSUMER_GROUP is required" }),
-  SQS_QUEUE_URL: z.string().nonempty({ message: "SQS_QUEUE_URL is required" }),
-  // PostgreSQL settings (for GitHub Projection Lambda)
-  // PGHOST: z.string().nonempty({ message: "PGHOST is required" }),
-  // PGPORT: z.preprocess((val) => (val ? parseInt(val) : 5432), z.number().int().positive()),
-  // PGUSER: z.string().nonempty({ message: "PGUSER is required" }),
-  // PGPASSWORD: z.string().nonempty({ message: "PGPASSWORD is required" }),
-  // PGDATABASE: z.string().nonempty({ message: "PGDATABASE is required" }),
-  // PGSSL: z.string().optional(),
+  BUCKET_NAME: z.string().nonempty(),
+  QUEUE_URL: z.string().nonempty(),
+  // Allow separate endpoints for S3 and SQS for local testing. If not provided, fall back to AWS_ENDPOINT.
+  S3_ENDPOINT: z.string().optional(),
+  SQS_ENDPOINT: z.string().optional(),
+  AWS_ENDPOINT: z.string().optional()
 });
 
-// --------------------
-// Load and validate configuration
-// --------------------
-function loadConfig() {
-  dotenv.config();
-  const parsed = configSchema.safeParse(process.env);
-  if (!parsed.success) {
-    console.error("Configuration error:", parsed.error.flatten().fieldErrors);
-    process.exit(1);
-  }
-  const conf = parsed.data;
-  // conf.PGSSL = conf.PGSSL === "true" ? { rejectUnauthorized: false } : false;
-  return conf;
+const config = configSchema.parse(process.env);
+
+// Log non-sensitive configuration details.
+function logConfig() {
+  console.log(JSON.stringify({
+    level: "info",
+    timestamp: new Date().toISOString(),
+    message: "Configuration loaded",
+    config: { BUCKET_NAME: config.BUCKET_NAME, QUEUE_URL: config.QUEUE_URL }
+  }));
 }
+logConfig();
 
-const config = loadConfig();
+// Use the separate endpoints if provided; otherwise use AWS_ENDPOINT.
+const s3 = new S3Client({ endpoint: config.S3_ENDPOINT || config.AWS_ENDPOINT });
+const sqs = new SQSClient({ endpoint: config.SQS_ENDPOINT || config.AWS_ENDPOINT });
 
-// --------------------------------------------------------------------------------------------------------------------
-// Tansu Consumer: Kafka -> SQS
-// --------------------------------------------------------------------------------------------------------------------
+const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));
 
-export function validateKafkaConfig(conf) {
-  if (!conf.BROKER_URL || !conf.TOPIC_NAME || !conf.CONSUMER_GROUP) {
-    console.error("Missing required Kafka configuration");
-    return false;
-  }
-  return true;
+function logInfo(message) {
+  console.log(JSON.stringify({ level: "info", timestamp: new Date().toISOString(), message }));
 }
 
-function getSQSClient() {
-  const client = new SQSClient({});
-  // In test/mock environments, add a dummy send if not defined.
-  if (typeof client.send !== "function") {
-    client.send = async (_command) => {
-      return { MessageId: "dummy-message" };
-    };
-  }
-  return client;
+function logError(message, error) {
+  console.error(JSON.stringify({ level: "error", timestamp: new Date().toISOString(), message, error: error ? error.toString() : undefined }));
 }
 
-export function buildSQSMessageParams(topic, partition, offset, messageValue) {
+async function listAndSortAllObjectVersions() {
+  let versions = [];
+  let params = { Bucket: config.BUCKET_NAME };
+  let response;
+  do {
+    response = await s3.send(new ListObjectVersionsCommand(params));
+    versions.push(...response.Versions);
+    params.KeyMarker = response.NextKeyMarker;
+    params.VersionIdMarker = response.NextVersionIdMarker;
+  } while (response.IsTruncated);
+
+  versions.sort((a, b) => new Date(a.LastModified) - new Date(b.LastModified));
+  return versions;
+}
+
+function buildSQSMessageParams(event) {
   return {
-    QueueUrl: config.SQS_QUEUE_URL,
-    MessageBody: messageValue,
-    MessageAttributes: {
-      Topic: { DataType: "String", StringValue: topic },
-      Partition: { DataType: "Number", StringValue: partition.toString() },
-      Offset: { DataType: "Number", StringValue: offset.toString() },
-    },
+    QueueUrl: config.QUEUE_URL,
+    MessageBody: JSON.stringify(event)
   };
 }
 
-export async function sendMessageToSQS(topic, partition, offset, messageValue) {
-  const sqsClient = getSQSClient();
-  const params = buildSQSMessageParams(topic, partition, offset, messageValue);
-  const command = new SendMessageCommand(params);
+async function sendEventToSqs(event) {
+  const params = buildSQSMessageParams(event);
   try {
-    console.log(`Sending message to SQS. params: ${JSON.stringify(params, null, 2)}`);
-    let response;
-    if (config.SQS_QUEUE_URL !== "https://sqs.region.amazonaws.com/123456789012/tansu-sqs-bridge-queue-local") {
-      response = await retryOperationExponential(() => sqsClient.send(command));
-      console.log(`Sent message to SQS. MessageId: ${response.MessageId}`);
-    } else {
-      response = { MessageId: "dummy-message" };
-      console.log(`FAKED send message to SQS for 'tansu-sqs-bridge-queue-local'. MessageId: ${response.MessageId}`);
-    }
-    return response;
+    const result = await retryOperationExponential(async () =>
+        await sqs.send(new SendMessageCommand(params))
+    );
+    logInfo(`Sent message to SQS, MessageId: ${result.MessageId}`);
   } catch (err) {
-    console.error("Error in sendMessageToSQS:", err);
+    logError("Failed to send message to SQS", err);
     throw err;
   }
 }
 
-export async function retryOperationExponential(operation, retries = 3, initialDelay = 500) {
-  let delay = initialDelay;
-  for (let attempt = 1; attempt <= retries; attempt++) {
-    try {
-      console.log(`Exponential attempt ${attempt}`);
-      return await operation();
-    } catch (error) {
-      if (attempt === retries) {
-        console.error("Exponential retries exhausted", error);
-        throw error;
-      }
-      await new Promise((resolve) => setTimeout(resolve, delay));
-      delay *= 2;
-    }
-  }
-}
-
-function sleep(ms) {
-  return new Promise((resolve) => setTimeout(resolve, ms));
-}
-
-export async function runConsumer() {
-  const kafka = new Kafka({
-    clientId: "tansu-sqs-consumer",
-    brokers: [config.BROKER_URL],
-  });
-  if (!validateKafkaConfig(config)) {
-    console.error("Invalid Kafka configuration. Exiting consumer.");
-    process.exit(1);
-  }
-  console.log(`Creating Kafka consumer with group ID ${config.CONSUMER_GROUP}`);
-  const consumer = kafka.consumer({ groupId: config.CONSUMER_GROUP });
-  // const sqsClient = getSQSClient();
-
-  console.log(`Connecting to Kafka Admin on ${config.BROKER_URL}`);
-  const admin = kafka.admin();
-  await admin.connect();
-  console.log(`Connected to Kafka Admin on ${config.BROKER_URL}`);
-
-  console.log(`Listing topics on ${config.BROKER_URL}`);
-  const existingTopics = await admin.listTopics();
-  if (!existingTopics.includes(config.TOPIC_NAME)) {
-    console.log(`Topic '${config.TOPIC_NAME}' does not exist.`);
-    if (config.USE_EXISTING_TOPIC !== "true") {
-      console.log(`Creating topic '${config.TOPIC_NAME}'`);
-      await admin.createTopics({ topics: [{ topic: config.TOPIC_NAME }] });
-      console.log(`Created topic '${config.TOPIC_NAME}'.`);
-    } else {
-      console.error(`CRITICAL: Topic '${config.TOPIC_NAME}' does not exist and USE_EXISTING_TOPIC == "true". Exiting.`);
-      process.exit(1);
-    }
-  } else {
-    console.log(`Topic '${config.TOPIC_NAME}' exists.`);
-  }
-
-  await admin.disconnect();
-
-  console.log(`Connecting to Kafka broker at ${config.BROKER_URL}`);
-  await consumer.connect();
-  console.log(`Connected to Kafka broker at ${config.BROKER_URL}`);
-
-  console.log(`Subscribing to Kafka topic ${config.TOPIC_NAME}`);
-  await consumer.subscribe({ topic: config.TOPIC_NAME, fromBeginning: true });
-  console.log(`Subscribed to Kafka topic ${config.TOPIC_NAME}`);
-
-  // Graceful shutdown on SIGINT
-  process.on("SIGINT", async () => {
-    console.log("Disconnecting Kafka consumer...");
-    // await consumer.disconnect();
-    process.exit(0);
-  });
-
-  console.log("Starting consumer loop...");
-  await consumer.run({
-    eachMessage: async ({ topic, partition, message }) => {
-      const key = message.key ? message.key.toString() : null;
-      const value = message.value ? message.value.toString() : "";
-      const offset = message.offset;
-      console.log(`Received message from topic=${topic} partition=${partition} offset=${offset}`);
-      console.debug("Message key:", key, "value:", value);
-
-      // Use the new helper to send message to SQS
-      try {
-        await sendMessageToSQS(topic, partition, offset, value);
-      } catch (err) {
-        console.error("Error sending message to SQS:", err);
-      }
-    },
-  });
-
-  // Health check endpoint
-  // eslint-disable-next-line sonarjs/x-powered-by
-  const app = express();
-  app.get("/", (req, res) => res.send("OK"));
-  console.log("Starting health check endpoint on port 8080");
-  app.listen(8080, () => console.log("Health check running on port 8080"));
-
-  // Infinite loop logging every second
-  let keepRunning = true;
-  while (keepRunning) {
-    console.log("Keep-alive loop active (logging every 30 seconds)");
-    try {
-      await sleep(30 * 1000);
-    } catch (e) {
-      console.error("Interrupted:", e);
-      keepRunning = false;
-      process.exit(1);
-    }
-  }
-}
-
-// ---------------------------------------------------------------------------------------------------------------------
-// SQS Lambda Handlers
-// ---------------------------------------------------------------------------------------------------------------------
-
-export async function loggingLambdaHandler(event) {
-  console.log("Logging Lambda received SQS event:", JSON.stringify(event, null, 2));
-  for (const record of event.Records) {
-    console.log("SQS Message:", record.body);
-  }
-  return { status: "logged" };
-}
-
-let pgClient;
-export async function getDbClient() {
-  if (!pgClient) {
-    pgClient = new PGClient({
-      host: config.PGHOST,
-      port: config.PGPORT,
-      user: config.PGUSER,
-      password: config.PGPASSWORD,
-      database: config.PGDATABASE,
-      ssl: config.PGSSL,
-    });
-    await pgClient.connect();
-    console.log("Connected to PostgreSQL database");
-  }
-  return pgClient;
-}
-export function resetDbClient() {
-  pgClient = undefined;
-}
-
-export function parseMessageBody(body) {
+function parseMessageBody(text) {
   try {
-    return JSON.parse(body);
-  } catch (err) {
-    console.error("Failed to parse message body", err);
+    return JSON.parse(text);
+  } catch (e) {
     return null;
   }
 }
 
-export function isValidResourceEvent(event) {
-  // Explicitly return a boolean value
-  return Boolean(event && event.resourceType && event.resourceId);
-}
-
-// DynamoDB: arn:aws:dynamodb:eu-west-2:541134664601:table/GithubProjections
-
-export async function updateProjection(client, resourceType, resourceId, state) {
-  const query = `
-    INSERT INTO github_projections (resource_id, resource_type, state, updated_at)
-    VALUES ($1, $2, $3, NOW())
-    ON CONFLICT (resource_id)
-    DO UPDATE SET state = EXCLUDED.state, updated_at = NOW();
-  `;
-  const values = [resourceId, resourceType, JSON.stringify(state)];
-  return await client.query(query, values);
+function validateConfig(cfg) {
+  return cfg.BUCKET_NAME && cfg.QUEUE_URL;
 }
 
-export async function githubProjectionLambdaHandler(event) {
-  console.log("GitHub Projection Lambda received event:", JSON.stringify(event, null, 2));
-
-  let client;
-  try {
-    client = await getDbClient();
-  } catch (error) {
-    console.error("Error connecting to Postgres", error);
-    throw error;
-  }
-
-  for (const record of event.Records) {
-    const bodyObj = parseMessageBody(record.body);
-    if (!bodyObj) continue;
-    const { resourceType, resourceId, state } = bodyObj;
-    if (!isValidResourceEvent({ resourceType, resourceId })) {
-      console.error("Missing resourceType or resourceId in event", record.body);
-      continue;
-    }
+async function retryOperationExponential(operation, retries = 3, delay = 100) {
+  let attempt = 0;
+  while (attempt < retries) {
     try {
-      await updateProjection(client, resourceType, resourceId, state);
-      console.log(`Updated projection for ${resourceType} ${resourceId}`);
+      return await operation();
     } catch (err) {
-      console.error("Error updating PostgreSQL projection", err);
+      attempt++;
+      if (attempt >= retries) {
+        throw err;
+      }
+      await sleep(delay * Math.pow(2, attempt));
     }
   }
-  return { status: "success" };
 }
 
-// ---------------------------------------------------------------------------------------------------------------------
-// Main CLI Function
-// ---------------------------------------------------------------------------------------------------------------------
-
-const HELP_TEXT = `Usage: node src/lib/main.js [--help|--simulate-projection|--tansu-consumer-to-sqs|--sqs-to-lambda-github-projection|--sqs-to-lambda-logger]`;
-
-export async function main(args = process.argv.slice(2)) {
-  if (args.includes("--help")) {
-    console.log(HELP_TEXT);
-    return;
-  }
-
-  if (args.includes("--tansu-consumer-to-sqs")) {
-    console.log("Starting Kafka consumer to send messages to SQS...");
-    await runConsumer();
-    return;
+export async function realtimeLambdaHandler(event) {
+  logInfo(`Received realtime event: ${JSON.stringify(event)}`);
+  for (const record of event.Records) {
+    const { s3 } = record;
+    const eventDetail = {
+      bucket: s3.bucket.name,
+      key: s3.object.key,
+      eventTime: record.eventTime,
+      versionId: s3.object.versionId,
+      sequencer: s3.object.sequencer
+    };
+    await sendEventToSqs(eventDetail);
   }
+}
 
-  if (args.includes("--sqs-to-lambda-logger")) {
-    const sampleEvent = {
-      Records: [{ body: "Sample message from Tansu consumer" }],
+export async function reseed() {
+  logInfo(`Starting reseed job for bucket ${config.BUCKET_NAME}`);
+  const versions = await listAndSortAllObjectVersions();
+  logInfo(`Processing ${versions.length} versions...`);
+  for (const version of versions) {
+    const event = {
+      bucket: config.BUCKET_NAME,
+      key: version.Key,
+      versionId: version.VersionId,
+      eventTime: version.LastModified
     };
-    console.log("Running Logging Lambda Handler with sample event...");
-    await loggingLambdaHandler(sampleEvent);
-    return;
+    await sendEventToSqs(event);
   }
+  logInfo('Reseed job complete.');
+}
 
-  if (args.includes("--sqs-to-lambda-github-projection")) {
-    const sampleEvent = {
-      Records: [
-        {
-          body: JSON.stringify({
-            resourceType: "repository",
-            resourceId: "tansu-sqs-bridge",
-            state: { stars: 285, forks: 6, openIssues: 14 },
-          }),
-        },
-      ],
-    };
-    console.log("Running GitHub Projection Lambda Handler with sample event...");
-    await githubProjectionLambdaHandler(sampleEvent);
+function healthCheckServer() {
+  const app = express();
+  app.get('/', (req, res) => res.send('S3 SQS Bridge OK'));
+  app.listen(8080, () => logInfo('Healthcheck available at :8080'));
+}
+
+async function main(args = process.argv.slice(2)) {
+  if (args.includes('--help')) {
+    console.log(`
+      Usage:
+      --realtime-lambda          Run realtime Lambda handler
+      --reseed                   Run full bucket reseed
+      --healthcheck              Run healthcheck server
+    `);
     return;
   }
 
-  console.log(`Run with: ${JSON.stringify(args)}`);
+  if (args.includes('--reseed')) {
+    await reseed();
+  } else if (args.includes('--realtime-lambda')) {
+    console.log('Realtime lambda handler requires AWS Lambda invocation context.');
+  } else if (args.includes('--healthcheck')) {
+    healthCheckServer();
+  } else {
+    console.log('Invalid or missing argument. Run with --help for usage.');
+  }
 }
 
-// --------------------
-// If run directly, call main() with CLI arguments.
-// Prevent execution during test runs by checking a VITEST flag.
-// --------------------
-if (process.argv[1] === fileURLToPath(import.meta.url) && !process.env.VITEST) {
-  main(process.argv.slice(2)).catch((err) => {
-    console.error("Fatal error:", err);
+if (import.meta.url.endsWith(process.argv[1])) {
+  main().catch((err) => {
+    logError('Fatal error in main execution', err);
     process.exit(1);
   });
 }
+
+export { main, parseMessageBody, buildSQSMessageParams, validateConfig, sendEventToSqs, retryOperationExponential, listAndSortAllObjectVersions };

commit 701de0dad86e43e5d3f604c5af784ddf981aabd5
Author: Antony @ Polycode <112443706+Antony-at-Polycode@users.noreply.github.com>
Date:   Mon Mar 17 00:15:12 2025 +0000

    Initial commit

diff --git a/src/lib/main.js b/src/lib/main.js
new file mode 100755
index 0000000..2278396
--- /dev/null
+++ b/src/lib/main.js
@@ -0,0 +1,383 @@
+#!/usr/bin/env node
+// src/lib/main.js
+// Tansu SQS Bridge - Aligned with our mission statement (v0.1.5)
+
+// Ensure NODE_ENV is set to development by default for local/test runs
+process.env.NODE_ENV = process.env.NODE_ENV || "development";
+
+import { fileURLToPath } from "url";
+import process from "process";
+import dotenv from "dotenv";
+import { z } from "zod";
+import { Kafka } from "kafkajs";
+import { SQSClient, SendMessageCommand } from "@aws-sdk/client-sqs";
+import pkg from "pg";
+import express from "express";
+
+const { Client: PGClient } = pkg;
+
+// --------------------
+// For test or development environment, supply default env values to avoid configuration errors.
+// In production, ensure all required environment variables are set.
+// --------------------
+if (process.env.VITEST || process.env.NODE_ENV === "development") {
+  process.env.SQS_QUEUE_URL = process.env.SQS_QUEUE_URL || "https://sqs.eu-west-2.amazonaws.com/000000000000/test";
+  process.env.BROKER_URL = process.env.BROKER_URL || "localhost:9092";
+  process.env.TOPIC_NAME = process.env.TOPIC_NAME || "test";
+  process.env.USE_EXISTING_TOPIC = process.env.USE_EXISTING_TOPIC || "false";
+  process.env.CONSUMER_GROUP = process.env.CONSUMER_GROUP || "tansu-sqs-bridge-group";
+  // process.env.PGHOST = process.env.PGHOST || "localhost";
+  // process.env.PGUSER = process.env.PGUSER || "test";
+  // process.env.PGPASSWORD = process.env.PGPASSWORD || "test";
+  // process.env.PGDATABASE = process.env.PGDATABASE || "test";
+  // process.env.PGPORT = process.env.PGPORT || "5432";
+}
+
+// --------------------
+// Environment configuration schema using zod
+// --------------------
+const configSchema = z.object({
+  // Kafka settings
+  BROKER_URL: z.string().nonempty({ message: "BROKER_URL is required" }),
+  TOPIC_NAME: z.string().nonempty({ message: "TOPIC_NAME is required" }),
+  USE_EXISTING_TOPIC: z.string().nonempty({ message: "USE_EXISTING_TOPIC is required" }),
+  CONSUMER_GROUP: z.string().nonempty({ message: "CONSUMER_GROUP is required" }),
+  SQS_QUEUE_URL: z.string().nonempty({ message: "SQS_QUEUE_URL is required" }),
+  // PostgreSQL settings (for GitHub Projection Lambda)
+  // PGHOST: z.string().nonempty({ message: "PGHOST is required" }),
+  // PGPORT: z.preprocess((val) => (val ? parseInt(val) : 5432), z.number().int().positive()),
+  // PGUSER: z.string().nonempty({ message: "PGUSER is required" }),
+  // PGPASSWORD: z.string().nonempty({ message: "PGPASSWORD is required" }),
+  // PGDATABASE: z.string().nonempty({ message: "PGDATABASE is required" }),
+  // PGSSL: z.string().optional(),
+});
+
+// --------------------
+// Load and validate configuration
+// --------------------
+function loadConfig() {
+  dotenv.config();
+  const parsed = configSchema.safeParse(process.env);
+  if (!parsed.success) {
+    console.error("Configuration error:", parsed.error.flatten().fieldErrors);
+    process.exit(1);
+  }
+  const conf = parsed.data;
+  // conf.PGSSL = conf.PGSSL === "true" ? { rejectUnauthorized: false } : false;
+  return conf;
+}
+
+const config = loadConfig();
+
+// --------------------------------------------------------------------------------------------------------------------
+// Tansu Consumer: Kafka -> SQS
+// --------------------------------------------------------------------------------------------------------------------
+
+export function validateKafkaConfig(conf) {
+  if (!conf.BROKER_URL || !conf.TOPIC_NAME || !conf.CONSUMER_GROUP) {
+    console.error("Missing required Kafka configuration");
+    return false;
+  }
+  return true;
+}
+
+function getSQSClient() {
+  const client = new SQSClient({});
+  // In test/mock environments, add a dummy send if not defined.
+  if (typeof client.send !== "function") {
+    client.send = async (_command) => {
+      return { MessageId: "dummy-message" };
+    };
+  }
+  return client;
+}
+
+export function buildSQSMessageParams(topic, partition, offset, messageValue) {
+  return {
+    QueueUrl: config.SQS_QUEUE_URL,
+    MessageBody: messageValue,
+    MessageAttributes: {
+      Topic: { DataType: "String", StringValue: topic },
+      Partition: { DataType: "Number", StringValue: partition.toString() },
+      Offset: { DataType: "Number", StringValue: offset.toString() },
+    },
+  };
+}
+
+export async function sendMessageToSQS(topic, partition, offset, messageValue) {
+  const sqsClient = getSQSClient();
+  const params = buildSQSMessageParams(topic, partition, offset, messageValue);
+  const command = new SendMessageCommand(params);
+  try {
+    console.log(`Sending message to SQS. params: ${JSON.stringify(params, null, 2)}`);
+    let response;
+    if (config.SQS_QUEUE_URL !== "https://sqs.region.amazonaws.com/123456789012/tansu-sqs-bridge-queue-local") {
+      response = await retryOperationExponential(() => sqsClient.send(command));
+      console.log(`Sent message to SQS. MessageId: ${response.MessageId}`);
+    } else {
+      response = { MessageId: "dummy-message" };
+      console.log(`FAKED send message to SQS for 'tansu-sqs-bridge-queue-local'. MessageId: ${response.MessageId}`);
+    }
+    return response;
+  } catch (err) {
+    console.error("Error in sendMessageToSQS:", err);
+    throw err;
+  }
+}
+
+export async function retryOperationExponential(operation, retries = 3, initialDelay = 500) {
+  let delay = initialDelay;
+  for (let attempt = 1; attempt <= retries; attempt++) {
+    try {
+      console.log(`Exponential attempt ${attempt}`);
+      return await operation();
+    } catch (error) {
+      if (attempt === retries) {
+        console.error("Exponential retries exhausted", error);
+        throw error;
+      }
+      await new Promise((resolve) => setTimeout(resolve, delay));
+      delay *= 2;
+    }
+  }
+}
+
+function sleep(ms) {
+  return new Promise((resolve) => setTimeout(resolve, ms));
+}
+
+export async function runConsumer() {
+  const kafka = new Kafka({
+    clientId: "tansu-sqs-consumer",
+    brokers: [config.BROKER_URL],
+  });
+  if (!validateKafkaConfig(config)) {
+    console.error("Invalid Kafka configuration. Exiting consumer.");
+    process.exit(1);
+  }
+  console.log(`Creating Kafka consumer with group ID ${config.CONSUMER_GROUP}`);
+  const consumer = kafka.consumer({ groupId: config.CONSUMER_GROUP });
+  // const sqsClient = getSQSClient();
+
+  console.log(`Connecting to Kafka Admin on ${config.BROKER_URL}`);
+  const admin = kafka.admin();
+  await admin.connect();
+  console.log(`Connected to Kafka Admin on ${config.BROKER_URL}`);
+
+  console.log(`Listing topics on ${config.BROKER_URL}`);
+  const existingTopics = await admin.listTopics();
+  if (!existingTopics.includes(config.TOPIC_NAME)) {
+    console.log(`Topic '${config.TOPIC_NAME}' does not exist.`);
+    if (config.USE_EXISTING_TOPIC !== "true") {
+      console.log(`Creating topic '${config.TOPIC_NAME}'`);
+      await admin.createTopics({ topics: [{ topic: config.TOPIC_NAME }] });
+      console.log(`Created topic '${config.TOPIC_NAME}'.`);
+    } else {
+      console.error(`CRITICAL: Topic '${config.TOPIC_NAME}' does not exist and USE_EXISTING_TOPIC == "true". Exiting.`);
+      process.exit(1);
+    }
+  } else {
+    console.log(`Topic '${config.TOPIC_NAME}' exists.`);
+  }
+
+  await admin.disconnect();
+
+  console.log(`Connecting to Kafka broker at ${config.BROKER_URL}`);
+  await consumer.connect();
+  console.log(`Connected to Kafka broker at ${config.BROKER_URL}`);
+
+  console.log(`Subscribing to Kafka topic ${config.TOPIC_NAME}`);
+  await consumer.subscribe({ topic: config.TOPIC_NAME, fromBeginning: true });
+  console.log(`Subscribed to Kafka topic ${config.TOPIC_NAME}`);
+
+  // Graceful shutdown on SIGINT
+  process.on("SIGINT", async () => {
+    console.log("Disconnecting Kafka consumer...");
+    // await consumer.disconnect();
+    process.exit(0);
+  });
+
+  console.log("Starting consumer loop...");
+  await consumer.run({
+    eachMessage: async ({ topic, partition, message }) => {
+      const key = message.key ? message.key.toString() : null;
+      const value = message.value ? message.value.toString() : "";
+      const offset = message.offset;
+      console.log(`Received message from topic=${topic} partition=${partition} offset=${offset}`);
+      console.debug("Message key:", key, "value:", value);
+
+      // Use the new helper to send message to SQS
+      try {
+        await sendMessageToSQS(topic, partition, offset, value);
+      } catch (err) {
+        console.error("Error sending message to SQS:", err);
+      }
+    },
+  });
+
+  // Health check endpoint
+  // eslint-disable-next-line sonarjs/x-powered-by
+  const app = express();
+  app.get("/", (req, res) => res.send("OK"));
+  console.log("Starting health check endpoint on port 8080");
+  app.listen(8080, () => console.log("Health check running on port 8080"));
+
+  // Infinite loop logging every second
+  let keepRunning = true;
+  while (keepRunning) {
+    console.log("Keep-alive loop active (logging every 30 seconds)");
+    try {
+      await sleep(30 * 1000);
+    } catch (e) {
+      console.error("Interrupted:", e);
+      keepRunning = false;
+      process.exit(1);
+    }
+  }
+}
+
+// ---------------------------------------------------------------------------------------------------------------------
+// SQS Lambda Handlers
+// ---------------------------------------------------------------------------------------------------------------------
+
+export async function loggingLambdaHandler(event) {
+  console.log("Logging Lambda received SQS event:", JSON.stringify(event, null, 2));
+  for (const record of event.Records) {
+    console.log("SQS Message:", record.body);
+  }
+  return { status: "logged" };
+}
+
+let pgClient;
+export async function getDbClient() {
+  if (!pgClient) {
+    pgClient = new PGClient({
+      host: config.PGHOST,
+      port: config.PGPORT,
+      user: config.PGUSER,
+      password: config.PGPASSWORD,
+      database: config.PGDATABASE,
+      ssl: config.PGSSL,
+    });
+    await pgClient.connect();
+    console.log("Connected to PostgreSQL database");
+  }
+  return pgClient;
+}
+export function resetDbClient() {
+  pgClient = undefined;
+}
+
+export function parseMessageBody(body) {
+  try {
+    return JSON.parse(body);
+  } catch (err) {
+    console.error("Failed to parse message body", err);
+    return null;
+  }
+}
+
+export function isValidResourceEvent(event) {
+  // Explicitly return a boolean value
+  return Boolean(event && event.resourceType && event.resourceId);
+}
+
+// DynamoDB: arn:aws:dynamodb:eu-west-2:541134664601:table/GithubProjections
+
+export async function updateProjection(client, resourceType, resourceId, state) {
+  const query = `
+    INSERT INTO github_projections (resource_id, resource_type, state, updated_at)
+    VALUES ($1, $2, $3, NOW())
+    ON CONFLICT (resource_id)
+    DO UPDATE SET state = EXCLUDED.state, updated_at = NOW();
+  `;
+  const values = [resourceId, resourceType, JSON.stringify(state)];
+  return await client.query(query, values);
+}
+
+export async function githubProjectionLambdaHandler(event) {
+  console.log("GitHub Projection Lambda received event:", JSON.stringify(event, null, 2));
+
+  let client;
+  try {
+    client = await getDbClient();
+  } catch (error) {
+    console.error("Error connecting to Postgres", error);
+    throw error;
+  }
+
+  for (const record of event.Records) {
+    const bodyObj = parseMessageBody(record.body);
+    if (!bodyObj) continue;
+    const { resourceType, resourceId, state } = bodyObj;
+    if (!isValidResourceEvent({ resourceType, resourceId })) {
+      console.error("Missing resourceType or resourceId in event", record.body);
+      continue;
+    }
+    try {
+      await updateProjection(client, resourceType, resourceId, state);
+      console.log(`Updated projection for ${resourceType} ${resourceId}`);
+    } catch (err) {
+      console.error("Error updating PostgreSQL projection", err);
+    }
+  }
+  return { status: "success" };
+}
+
+// ---------------------------------------------------------------------------------------------------------------------
+// Main CLI Function
+// ---------------------------------------------------------------------------------------------------------------------
+
+const HELP_TEXT = `Usage: node src/lib/main.js [--help|--simulate-projection|--tansu-consumer-to-sqs|--sqs-to-lambda-github-projection|--sqs-to-lambda-logger]`;
+
+export async function main(args = process.argv.slice(2)) {
+  if (args.includes("--help")) {
+    console.log(HELP_TEXT);
+    return;
+  }
+
+  if (args.includes("--tansu-consumer-to-sqs")) {
+    console.log("Starting Kafka consumer to send messages to SQS...");
+    await runConsumer();
+    return;
+  }
+
+  if (args.includes("--sqs-to-lambda-logger")) {
+    const sampleEvent = {
+      Records: [{ body: "Sample message from Tansu consumer" }],
+    };
+    console.log("Running Logging Lambda Handler with sample event...");
+    await loggingLambdaHandler(sampleEvent);
+    return;
+  }
+
+  if (args.includes("--sqs-to-lambda-github-projection")) {
+    const sampleEvent = {
+      Records: [
+        {
+          body: JSON.stringify({
+            resourceType: "repository",
+            resourceId: "tansu-sqs-bridge",
+            state: { stars: 285, forks: 6, openIssues: 14 },
+          }),
+        },
+      ],
+    };
+    console.log("Running GitHub Projection Lambda Handler with sample event...");
+    await githubProjectionLambdaHandler(sampleEvent);
+    return;
+  }
+
+  console.log(`Run with: ${JSON.stringify(args)}`);
+}
+
+// --------------------
+// If run directly, call main() with CLI arguments.
+// Prevent execution during test runs by checking a VITEST flag.
+// --------------------
+if (process.argv[1] === fileURLToPath(import.meta.url) && !process.env.VITEST) {
+  main(process.argv.slice(2)).catch((err) => {
+    console.error("Fatal error:", err);
+    process.exit(1);
+  });
+}
