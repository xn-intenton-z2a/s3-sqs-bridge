src/index.js
==== Content of src/index.js ====
// src/index.js
// This file is intentionally left empty.src/lib/main.js
==== Content of src/lib/main.js ====
#!/usr/bin/env node
// src/lib/main.js

import { fileURLToPath } from "url";
import process from "process";
import dotenv from "dotenv";
import { z } from "zod";
import { Kafka } from "kafkajs";
import { SQSClient, SendMessageCommand } from "@aws-sdk/client-sqs";
// import { Client as PGClient } from "pg";
import pkg from "pg";

const { Client: PGClient } = pkg;

// --------------------
// For test environment, supply default env values to avoid configuration errors.
// --------------------
if (process.env.VITEST) {
  process.env.SQS_QUEUE_URL = process.env.SQS_QUEUE_URL || "test-sqs-queue-url";
  process.env.PGHOST = process.env.PGHOST || "localhost";
  process.env.PGUSER = process.env.PGUSER || "test";
  process.env.PGPASSWORD = process.env.PGPASSWORD || "test";
  process.env.PGDATABASE = process.env.PGDATABASE || "test";
}

// --------------------
// Load environment and validate configuration
// --------------------
dotenv.config();

const configSchema = z.object({
  // Kafka settings
  BROKER_URL: z.string().default("localhost:9092"),
  TOPIC_NAME: z.string().default("test"),
  CONSUMER_GROUP: z.string().default("tansu-sqs-bridge-group"),

  // AWS SQS settings
  SQS_QUEUE_URL: z.string().nonempty({ message: "SQS_QUEUE_URL is required" }),

  // PostgreSQL settings (for GitHub Projection Lambda)
  PGHOST: z.string().nonempty({ message: "PGHOST is required" }),
  PGPORT: z.preprocess((val) => (val ? parseInt(val) : 5432), z.number().int().positive()),
  PGUSER: z.string().nonempty({ message: "PGUSER is required" }),
  PGPASSWORD: z.string().nonempty({ message: "PGPASSWORD is required" }),
  PGDATABASE: z.string().nonempty({ message: "PGDATABASE is required" }),
  PGSSL: z.string().optional(),
});

const parsed = configSchema.safeParse(process.env);
if (!parsed.success) {
  console.error("Configuration error:", parsed.error.flatten().fieldErrors);
  process.exit(0);
}
const config = parsed.data;
config.PGSSL = config.PGSSL === "true" ? { rejectUnauthorized: false } : false;

// --------------------
// Logger utility
// --------------------
const log = {
  info: (...args) => console.log("[INFO]", ...args),
  debug: (...args) => console.debug("[DEBUG]", ...args),
  error: (...args) => console.error("[ERROR]", ...args),
};

// --------------------
// PostgreSQL client utility (for GitHub Projection Lambda)
// --------------------
let pgClient;
async function getDbClient() {
  if (!pgClient) {
    pgClient = new PGClient({
      host: config.PGHOST,
      port: config.PGPORT,
      user: config.PGUSER,
      password: config.PGPASSWORD,
      database: config.PGDATABASE,
      ssl: config.PGSSL,
    });
    await pgClient.connect();
    log.info("Connected to PostgreSQL database");
  }
  return pgClient;
}
export function resetDbClient() {
  pgClient = undefined;
}

// --------------------
// AWS SQS client utility
// --------------------
function getSQSClient() {
  return new SQSClient({});
}

// --------------------
// Tansu Consumer: Kafka -> SQS
// --------------------
async function runConsumer() {
  const kafka = new Kafka({
    clientId: "tansu-sqs-consumer",
    brokers: [config.BROKER_URL],
  });
  const consumer = kafka.consumer({ groupId: config.CONSUMER_GROUP });
  const sqsClient = getSQSClient();

  await consumer.connect();
  log.info(`Connected to Kafka broker at ${config.BROKER_URL}`);

  await consumer.subscribe({ topic: config.TOPIC_NAME, fromBeginning: true });
  log.info(`Subscribed to Kafka topic ${config.TOPIC_NAME}`);

  await consumer.run({
    eachMessage: async ({ topic, partition, message }) => {
      const key = message.key ? message.key.toString() : null;
      const value = message.value ? message.value.toString() : "";
      const offset = message.offset;
      log.info(`Received message from topic=${topic} partition=${partition} offset=${offset}`);
      log.debug("Message key:", key, "value:", value);

      const params = {
        QueueUrl: config.SQS_QUEUE_URL,
        MessageBody: value,
        MessageAttributes: {
          Topic: { DataType: "String", StringValue: topic },
          Partition: { DataType: "Number", StringValue: partition.toString() },
          Offset: { DataType: "Number", StringValue: offset.toString() },
        },
      };

      try {
        const command = new SendMessageCommand(params);
        const response = await sqsClient.send(command);
        log.info(`Sent message to SQS. MessageId: ${response.MessageId}`);
      } catch (err) {
        log.error("Error sending message to SQS:", err);
      }
    },
  });

  // Graceful shutdown on SIGINT
  process.on("SIGINT", async () => {
    log.info("Disconnecting Kafka consumer...");
    await consumer.disconnect();
    process.exit(0);
  });
}

// --------------------
// Lambda Handlers (for AWS deployment)
// --------------------

async function loggingLambdaHandler(event) {
  log.info("Logging Lambda received SQS event:", JSON.stringify(event, null, 2));
  for (const record of event.Records) {
    log.info("SQS Message:", record.body);
  }
  return { status: "logged" };
}

async function githubProjectionLambdaHandler(event) {
  log.info("GitHub Projection Lambda received event:", JSON.stringify(event, null, 2));

  let client;
  try {
    client = await getDbClient();
  } catch (error) {
    log.error("Error connecting to Postgres", error);
    throw error;
  }

  for (const record of event.Records) {
    let body;
    try {
      body = JSON.parse(record.body);
    } catch (error) {
      log.error("Error parsing record body", error);
      continue;
    }
    const { resourceType, resourceId, state } = body;
    if (!resourceType || !resourceId) {
      log.error("Missing resourceType or resourceId in event", body);
      continue;
    }

    const query = `
      INSERT INTO github_projections (resource_id, resource_type, state, updated_at)
      VALUES ($1, $2, $3, NOW())
      ON CONFLICT (resource_id)
      DO UPDATE SET state = EXCLUDED.state, updated_at = NOW();
    `;
    const values = [resourceId, resourceType, JSON.stringify(state)];
    try {
      await client.query(query, values);
      log.info(`Updated projection for ${resourceType} ${resourceId}`);
    } catch (err) {
      log.error("Error updating PostgreSQL projection", err);
    }
  }
  return { status: "success" };
}

// --------------------
// Main CLI Function
// --------------------
export async function main(args = process.argv.slice(2)) {
  if (args.includes("--help")) {
    console.log("Usage: node src/lib/main.js [--help|--diagnostics|--demo|--tansu-consumer-to-sqs|--sqs-to-lambda-github-projection|--tansu-consumer-to-sqs]");
    return;
  }

  if (args.includes("--diagnostics")) {
    console.log("Diagnostics: All systems operational.");
    return;
  }

  if (args.includes("--demo")) {
    console.log("Demo output: This is a demo run.");
    console.log("This is a demo output. Replace with real implementation as needed.");
    return;
  }

  if (args.includes("--tansu-consumer-to-sqs")) {
    console.log("Starting Kafka consumer to send messages to SQS...");
    await runConsumer();
    return;
  }

  if (args.includes("--sqs-to-lambda-github-projection")) {
    const sampleEvent = {
      Records: [
        {
          body: JSON.stringify({
            resourceType: "repository",
            resourceId: "tansu-sqs-bridge",
            state: { stars: 285, forks: 6, openIssues: 14 },
          }),
        },
      ],
    };
    console.log("Running GitHub Projection Lambda Handler with sample event...");
    await githubProjectionLambdaHandler(sampleEvent);
    return;
  }

  if (args.includes("--tansu-consumer-to-sqs")) {
    const sampleEvent = {
      Records: [{ body: "Sample message from Tansu consumer" }],
    };
    console.log("Running Tansu Lambda Handler with sample event...");
    await loggingLambdaHandler(sampleEvent);
    return;
  }

  console.log(`Run with: ${JSON.stringify(args)}`);
  return;
}

// Export diagnostics function for external use/testing.
export function diagnostics() {
  return "Diagnostics: All systems operational.";
}

// --------------------
// If run directly, call main() with CLI arguments.
// Prevent execution during test runs by checking a VITEST flag.
// --------------------
if (process.argv[1] === fileURLToPath(import.meta.url) && !process.env.VITEST) {
  main(process.argv.slice(2)).catch((err) => {
    log.error("Fatal error:", err);
    process.exit(1);
  });
}
